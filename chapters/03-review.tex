\chapter{A Review of the State of the Art}\label{ch:review}

This Chapter provides a detailed examination of various hash table implementations and optimizations, focusing on their design principles, performance implications, and practical applications.
It is structured into several sections, each addressing different designs.
This Chapter is not an exhaustive list of hash table designs: we avoid detailing the designs which are generally considered superseded, and instead focus on the most relevant ones.

We begin by exploring CPython's built-in sequential hash table, already in wide use in the Python community.
Its core design principles, and the limitations imposed by the tradeoffs of free-threading CPython are described.
There are also included some general remarks about hash computation in Python.

We then explore concurrent hash table designs, beginning with Maier's hash table which was the main source of design inspiration for \texttt{AtomicDict}.
Important differences with the hashing techniques used in CPython's \texttt{dict} are highlighted.
We then continue exploring novel hash table designs which are all based on Maier's design.
It has been found to be an important milestone in the literature.
Two designs are explored: Bolt and DRAMHiT, both of which contributed some design decisions in \texttt{AtomicDict}, albeit less prominently.

We conclude with some remarks on the feasibility of completely lock-free hash tables, finding that they are in fact not practical due to the limitations of currently available hardware.


\section{Python's Sequential Hash Table}\label{sec:python-dict-review}

Python's built-in hash table, already introduced in Section~\ref{sec:dict-intro}, is a sequential hash table with important ramifications inside the CPython interpreter.
This Section provides an in-depth examination of CPython's \texttt{dict}, highlighting its design principles and operational mechanisms.

One notable optimization is key sharing, which significantly reduces memory consumption by storing the set of keys separately from their associated values.
Furthermore, the compact dictionary design, proposed by Raymond Hettinger, has significantly improved the previous implementation by optimizing the overall layout for enhanced performance and space utilization.

We find some relevance of these design choices for \texttt{AtomicDict} as well.
Later, we also highlight the changes that have been made to \texttt{dict} in order to support free-threading, but here we conclude that those changes overly favor the single-threaded use-cases, which are less relevant for \texttt{AtomicDict}.


\subsection{Key Sharing}\label{subsec:key-sharing}

Recall that CPython uses hash tables to store the attributes of objects.
It is immediate to see that different instances of the same class will generally have the same keys in their respective hash tables.
A relatively simple optimization is thus to store the set of keys separately.

This optimization is only applied for those hash tables that are used for class instances.
When creating a \texttt{dict} directly in Python code, CPython will not create a hash table with key sharing.

This creates slight performance overhead, compared to using a combined table for keys and values.
Indeed, data locality is slightly diminished, an important factor for cache access optimization.
It was measured, though, that the overall decrease in memory usage, more than compensated for this loss of locality.
Further details are available in Shannon's proposal~\cite{pep412}.

This optimization was not interesting for use in \texttt{AtomicDict}, because it was deemed unlikely that the hash tables shared among multiple threads share the same set of keys.


\subsection{Compact Dictionary}\label{subsec:compact-dict}

In late 2012, Raymond Hettinger proposed a compact, ordered implementation of CPython's \texttt{dict}, in~\cite{hettinger-dict}.
The design, which is still \texttt{dict}'s main design as of the time of writing, is based on the observation that the previous one was memory-inefficient.
Hettinger noted that the table in which the items were stored, was unnecessarily wide: it required 24 bytes for storing the key pointer, the value pointer, and the hash value.
In combination with the necessity of keeping the table sufficiently sparse (in order to cope with collisions), it generated a large amount of unused space.
He instead proposed having two separate tables: one sparse table, called the index, and another, dense table, called the entries table, which the index table refers to.

Insertion order is preserved because iterations read the entries table, and new items are appended to that table.
The actual hash table is the sparse index, which can fit into two cache lines for tables of size $m \leq 128$.

To further reduce memory usage, the size of index slots will vary depending on the size of the key set: when it is small there is no need to reference the entries table with 64-bit numbers.
Instead, the slot size is chosen dynamically, from a lower bound of 1-byte, when $m \leq 128$, to a maximum of 8 bytes for very large hash tables.

In order to accomodate Key Sharing (see above), the entries table may be further split into a key table, and a values table.

Splitting the sparse index table from the dense entries table, improves on both the storage space and the iteration speed.
In fact, if we call $\alpha$ the load factor of the dictionary (i.e.\ the number of occupied slots over the total number of slots), there not need be $(1 - \alpha)m$ empty 24-bytes entries for the unused slots in the hash table.
Instead, there will be $(1 - \alpha)m$ empty $O(8)$-bytes slots in the index.
That is, the entries table will have a load factor of $\sim$100\%: entirely filled, except for deleted entries.
When iterating over the keys in the dictionary, the dense entries table will serve as the basis for the iteration, instead of the sparse index.
Thus, reading unoccupied slots is avoided, and the memory bandwidth required for an iteration is reduced.

As the set of keys becomes larger than the maximum allowed load factor $\alpha_{\max} = 2/3$, the resizing process is initiated, which does not modify the entries table, but only the smaller index.
(Resizing is also termed \emph{migrating} in the literature.)
CPython's \texttt{dict} does not reduce its size after a large-enough sequence of deletions, that is, $\nexists\alpha_{\min}$.


\subsection{Python Hashing}\label{subsec:python-generated-hash-values}

CPython takes a controversial approach to hashing that departs from the standard literature, but it is grounded in the concrete usage of their hash tables.
Quoting from~\cite{dict-comment-hash}:
\begin{quote}
	Most hash schemes depend on having a ``good'' hash function, in the sense of simulating randomness.
	Python doesn't:  its most important hash functions (for ints) are very regular in common cases:
	\begin{verbatim}
		>>>[hash(i) for i in range(4)]
	  	[0, 1, 2, 3]
	\end{verbatim}
	This isn't necessarily bad!
	To the contrary, in a table of size $2^i$, taking the low-order $i$ bits as the initial table index is extremely fast, and there are no collisions at all for dicts indexed by a contiguous range of ints.
	So this gives better-than-random behavior in common cases, and that's very desirable.
\end{quote}

The hash functions employed for built-in CPython data types enjoy the following properties:
\begin{enumerate}
	\item objects which compare equal have the same hash value;
	\item one object always produces the same hash value;
	\item the computed hashes distribution is uniform, for a uniformly distributed set of values to be hashed;
	\item their running time complexity is $O(1)$;
	\item they don't necessitate holding locks, nor doing I/O, or other operations which may be generally considered improper for a hash function.
\end{enumerate}
The reader will probably have noticed how these properties are generally true for all hash functions, with properties (1) and (2) being fundamental for any hashing scheme to work, properties (4), and (5) being trivial for all hash functions known in the literature, and property (3) being what is usually the concern behind the implementation of hash functions.

With hash functions being so foundational to the implementation of hash tables, it is important to make one explicit remark.
There is no guarantee that the hash functions we indirectly employ (and that CPython's \texttt{dict} employs, too) enjoy the above properties.
That is to say, a hash function written in Python code may perform any arbitrary operation.
For instance, it may very well acquire a lock, initiate a request to a remote machine on the network, read a large file from disk, or run an $O(n!)$ algorithm to produce the result.

Of course, such behaviors would be completely unexpected of a hash function.
We will therefore resort to assume that those properties hold, as we have no control over those functions.


\subsection{With the Free-Threading Changes}\label{subsec:dict-free-threading}

Without the GIL, concurrent accesses to a sequential hash table such as \texttt{dict} may yield inconsistent program states, and lead to interpreter crashes.
Free-threading CPython requires that concurrent accesses to \texttt{dict} must be synchronized through a mutex.

Size retrieval of a \texttt{dict} is exempt from this requirement, since the size field itself is maintained correct with the \texttt{dict}'s mutex, and is thus easily available for concurrent reading.

Two operations are also partially exempt.
They are:
\begin{enumerate}
	\item \texttt{{dict[key]}}, i.e.\ a lookup operation; and
	\item reading the next value from a \texttt{dict} iterator, which is also implemented with a lookup operation.
\end{enumerate}
These two operations may optimistically run without locking the \texttt{dict}'s mutex, but are sometimes required to fall back to a slow path that requires locking.
The slow path is executed when some mutation on the \texttt{dict} was performed concurrently.

Let us consider two cases involving a lookup (or eq.\ an iteration), and another concurrent mutation.
Note that concurrent mutations are always obstructed by other concurrent mutations.

\paragraph{Lookup and Update.}
Suppose two threads $t_l$ and $t_u$ are respectively reading a key $k$ and updating its associated value $v$, in \texttt{dict} $H$.
The following can happen:
\begin{enumerate}
	\item $t_u$ acquires the lock of $H$;
	\item $t_l$ and $t_u$ find the position of $k$ in $H$;
	\item $t_l$ reads $v$;
	\item $t_u$ updates the value associated with $k$, from $v$ to $v'$;
	\item $t_u$ decrements the reference count of $v$;
	\item $t_u$ observes that $v$'s reference count is 0, and proceeds with a delayed free of $v$;
	\item $t_l$ tries to increment $v$'s reference count, but fails because it is 0;
	\item $t_l$ falls back to the slow path and is obstructed in the acquisition of $H$'s lock, which $t_u$ still holds;
	\item {\ldots}eventually{\ldots}
	\item $t_u$ releases $H$'s lock;
	\item w.l.o.g.\ assume that $t_l$ acquires the lock (it may be that another thread acquires it first, but later releases it, giving $t_l$ another chance to acquire it, infinitely often);
	\item $t_l$ restarts the lookup operation, and finds the position of $k$ in $H$;
	\item $t_l$ reads $v'$;
	\item $t_l$ successfully increments the reference count of $v'$ (in fact, the count could not be 0, because $v'$ is still referenced by $H$, and no mutations can happen in $H$ while $t_l$ is holding its lock);
	\item $t_l$ releases the lock of $H$, and returns $v'$.
\end{enumerate}

\paragraph{Lookup and Resize.}
Suppose two threads $t_l$ and $t_i$ are respectively reading a key $k$ and inserting another key $k'$, in \texttt{dict} $H$.
The following can happen:
\begin{enumerate}
	\item $t_i$ acquires the lock of $H$;
	\item $t_l$ finds the position of $k$ in $H$;
	\item $t_l$ reads $v$;
	\item $t_i$ notices that the current load factor $\alpha$ of $H$ is too high to perform the insertion;
	\item $t_i$ resizes $H$;
	\item $t_l$ notices that a concurrent resize has happened (let us avoid explicating how this check is performed);
	\item $t_l$ falls back to the slow path and is obstructed in the acquisition of $H$'s lock, which $t_i$ still holds;
	\item {\ldots}eventually{\ldots}
	\item $t_i$ inserts $k'$;
	\item $t_i$ releases $H$'s lock;
	\item w.l.o.g.\ assume that $t_l$ acquires the lock;
	\item $t_l$ restarts the lookup operation, and finds the position of $k$ in $H$;
	\item $t_l$ reads $v$;
	\item $t_l$ successfully increments the reference count of $v$;
	\item $t_l$ releases the lock of $H$, and returns $v$.
\end{enumerate}

As it can be seen from the above correct executions, CPython's \texttt{dict} does not exhibit the aforementioned natural parallelism: operations on distinct keys generate contention.
Not just contention actually: they generate obstruction.
Furthermore, this applies to \emph{any} two or more keys, differently from a fine-grained locking scheme like the one described in~\cite[\S13.2.2]{art-mp}.


\section{Maier's Concurrent Hash Table}\label{sec:maier-review}

In~\cite{maier}, Maier et al.\ describe a table migration process that crucially eliminates most of the contention that can arise from a migration.
In contrast with the standard literature (see~\cite[Figure~13.30]{art-mp}) the migration process described does not necessitate the acquisition of one or multiple locks by a single thread that then takes care of performing the resizing.
In fact, in the entire Chapter~13 of~\cite{art-mp}, resizing (eq.\ migrating) is described only in terms of a ``stop-the-world'' operation.

Maier's hash table implementation has an important difference when compared with Python's dictionary.
In the latter, the least-significant bits of the hash are used to determine the position in the linear array; that is, $d_0(x) = x \mod 2^s$, with $s$ being the logarithm base-2 of the dictionary size.
While in Maier's, the most-significant bits are used: $d_0(x) = x \gg (64 - s)$, where $\gg$ is a right-shift operation.
The reason why Maier chose the right-shift based scheme is so that the relative position of two keys which are in the same probe doesn't change as much as in the alternative, when the array is resized.

That is, consider a key $x$ s.t. $h(x) \mathbin{\&} 2^{s + 1} = 0$, and a key $y$ s.t.\ $h(y) \mathbin{\&} 2^{s + 1} = 1$, where $\mathbin{\&}$ is a bitwise-and operation.
If they were colliding before resizing from $s$ to $s + 1$ (i.e. $h(x) \mathbin{\&} (2^s - 1) = h(y) \mathbin{\&} (2^s - 1)$), they will not collide in the corresponding hash table of size $s + 1$.
But this creates an unexpected problem.
Consider the distance-0 position $d_0$ after the migration $d_0'(x) = d_0(x)$.
The $d_0$ position of $y$ will have changed from $d_0(y) = h(y) \mathbin{\&} (2^s - 1)$ to $d_0'(y) = h(y) \mathbin{\&} (2^{s+1} - 1)$.
In other words, $d_0'(y) = d_0(y) + 2^s$.
The problem with this behavior is that the position in the new array cannot be safely determined by looking at the prior array, without reading the actual hashes of each key in a probe.

This problem is avoided by choosing the most-significant bits scheme: $d_0(y) = h(y) \gg (64 - s)$, and $d_0'(y) = h(y) \gg (64 - s - 1)$.
In other words, $d_0'(y) = d_0(y) \cdot 2 + 1$.
While $d_0(x) = h(x) \gg (64 - s)$, and $d_0'(x) = h(x) \gg (64 - s - 1) \Rightarrow d_0'(x) = d_0(x) \cdot 2$.

It follows that the nodes of a cluster in the prior array will ``stay close to each other'' in the new array.
This behavior enables avoiding the necessity to exercise care in the moving of nodes from one generation of the hash table to the next, resulting in much less synchronization overhead.


\subsection{Complex Keys and Values}\label{subsec:maier-complex-keys-and-values}

The hash table of~\cite{maier} is limited to integer keys and values.
This is too severe a limitation for a Python hash table: it would in fact not be possible to directly handle C integers from Python code, and it would be very desirable to use other types of objects as well.

In order for Python code to interact with this limited hash table, the keys and values would need to be cast to Python \texttt{int} types.
While possible in principle, it also would entail a performance decrease, since the \texttt{int} objects themselves would be contended, even between lookups.
Two concurrent lookups over the same key $k$, would necessarily contend on the reference count of the associated \texttt{int} object.
Therefore, it would make no difference to store the \texttt{int} object pointers themselves in the hash table.

In~\cite[\S5.7]{maier}, Maier et al.\ note that it is possible to generalize their hash table to complex keys and values, albeit with some impact on performance.
The publicly available implementation advertises the availability of this functionality, thus some work followed the publication of the paper.
Notwithstanding, the feature seems not to be functioning.
The necessary compiler flag was explicitly disabled by default, and enabling it causes errors.
The authors of the library have not responded to inquiries by this author pertaining said functionality.
We have resolved to assume that it is not available.
We did not have the possibility to assess its performance impact.

The remarks made on memory management are resembling of mimalloc's approach to sharded, thread-free lists, described in~\cite{mimalloc} and employed in free-threading CPython.


\section{Bolt}\label{sec:bolt}

Building on the work described in the previous Section, E.\ Kahssay presented in his Master Thesis a concurrent hash table named Bolt that allegedly has even better performance.
It is based on the application of Robin Hood hashing, a modified linear probing scheme first described by Celis et al.\ in~\cite{robin-hood} where already inserted items can be displaced by newly inserted items so as to reduce the variance in probe length.
The technique enables the pruning of searches in linear probing.

This leads to a more complex insertion routine.
Consider the general situation in which a new item is being inserted.
Paraphrasing from~\cite{robin-hood}:
\begin{quote}
	Suppose that element $A$ is at location $l$, at distance $d_a$ from its $d_0$ slot, and $B$ is to be inserted.
	Suppose $B$ has been denied allocation of its first $d_b - 1$ choices, and we are now considering $l$, its $d_b$th choice.
\end{quote}
Then, the Robin Hood invariant maintains that:

\begin{tabular}{rp{8cm}}
	if $d_a \geq d_b$: & $A$ retains $l$, the insertion of $B$ continues by considering its $(d_b + 1)$st choice; \\
	if $d_a < d_b$: & $B$ displaces $A$ from $l$, the insertion procedure for $A$ is applied starting with its next $(d_a + 1)$st choice; \\
\end{tabular}

The name Robin Hood hashing is derived from an analogy with the popular character:
\begin{quote}
	This procedure of ``taking from the rich and giving to the poor'' gives rise to the name Robin Hood hashing.
	Like Robin Hood, it does not change the average wealth (mean probe length), only its distribution.
	It is clear that the principal effect is to substantially reduce the variance.
\end{quote}

This insertion scheme can involve more than one write to the hash table in order to insert one item: a non-trivial problem for a concurrent hash table.
Consider the linearizability property of concurrent data structures, where it is desirable that the mutation of one method is instantaneously applied.
This generally requires to have exactly one write to the shared data structure.
For instance, in Maier's implementation the insertion is carried out by a single atomic write into the hash table, and this satisfies the linearizability property.
Bolt satisfies this property by locking sections of the hash table, a technique known as fine-grained locking.
A thread inserting a new item first acquires all the relevant locks, and then proceeds with the Robin Hood insertion described.

Clearly, the lock-freedom property is lost in this process.
The performance characteristics presented in Kahssay's Thesis supposedly compensate for this.
Notwithstanding, an implementation was not provided, neither in the Thesis, nor upon request to Kahssay.
The author has thus been unable to confirm the results presented.

The Robin Hood scheme was considered interesting nonetheless, because it especially enhances the performance of lookups that fail.
In fact, with regular linear probing it is necessary to visit the entire probe before returning a failure, while in Robin Hood hashing, it is possible to prune the search by observing the distance of items in the probe.
The approach taken to Robin Hood hashing by this author is presented in Section~\ref{sec:lazy-robin-hood}.

\paragraph{Performance under contention.}
Contentious accesses are a common pattern in real-world usages of concurrent hash tables, as commonly known.
Bolt scheme of using fine-grained locking would seem especially subject to degradation in high-contention scenarios, as also noted by Kahssay himself.
Let us compare the approaches of Maier et al.\ \cite[\S8.4, ``Performance under contention'']{maier} and of Kahssay~\cite[\S7.4]{bolt} in evaluating these scenarios.
Differently from Maier, Kahssay only evaluates a Zipf distribution with a contention parameter $\theta = 0.75$.
In a Zipf distribution it can be that $\theta \in [0, 2]$, which are the bounds considered by Maier.
In fact, as noted by Maier, a little contention can help performance.
The experiment evaluated by Kahssay comprises a workload of 90\% lookups and 10\% updates.
In such a scenario, it happens that contention actually increases performance due to caching for 90\% of the workload.
The chosen scenario makes it unfeasible to properly compare Kahssay's and Maier's hash tables under contention.
It would have been much more interesting to review a scenario with 100\% updates and varying contention parameter $\theta$, as shown in Maier's work.
Without any distribution of Bolt, it is not possible to properly review its performance under contention.


\section{DRAMHiT}\label{sec:dramhit}

In~\cite{dramhit} a hash table that allegedly reaches the throughput of main memory is presented.
We raise here various doubts on this work and its underlying implementation.

The abuse of empiricism makes it difficult to verify their results, rendering them unreliable.
For instance, the observation that:
\begin{quote}
	[\ldots] on a fill factor of 75--80\%, lookup and insertion operations require only 1.3 cache line accesses per request on average.
	[\ldots] This is critical for reducing pressure on the memory subsystem.
\end{quote}
is not backed by any evidence, nor it is ever detailed how such measurements were made.
Thus, the measurements are found to be unreproducible.

The obsession with maintaining the highest attainable performance, has induced the authors of DRAMHiT to directly employ assembly code in their implementation, making it substantially less portable.
Given that the architectures supported by CPython are numerous, the present author necessitates code portability between different platforms and architectures; it was hard to draw inspiration from this implementation.
Also, considering the added burden of supporting several assembly code paths in order to support several architectures, this was considered a poor implementation choice.
In addition to the observations on portability, the employment of assembly code also renders the code much less readable and maintainable.

DRAMHiT operations design is largely based on the design of Maier's hash table.
Never throughout the paper the lock-freedom property of DRAMHiT is expressly mentioned.
We find that it is in fact not lock-free, albeit largely based on lock-free designs.
This is chiefly because of the design of~\cite[Algorithm~1]{dramhit}, where the prefetch engine is presented.
In fact, it can be easily seen that if one thread $t$ performs a lookup operation, but there are currently not enough pending operations to fill the prefetch window, $t$ is arbitrarily delayed.
Furthermore, if there were no running threads other than $t$, the prefetch window would never be filled, and $t$ would be blocked for an unbounded amount of time; even if it is the only thread running.
By the operations presented in the paper, it seems as though a single thread performing a single key lookup is not an effectively supported use-case, which is absurd.

Finally, the design choice that made the author most reluctant to accept the design of DRAMHiT was its asynchronous interface.
Only accessing prefetched memory is a very desirable characteristic, but in order to do so DRAMHiT chose to radically change the external interface of hash tables.
In fact, the very common operations enumerated in Section~\ref{sec:hash-tables} are not directly available.
Instead, those operations only entail a \emph{request} to the hash table.
The program needs to also check for the availability of the response before reading the result.
That is, a program as simple as \texttt{{v = H.lookup(k)}} has to become something along the lines of:
\begin{verbatim}
	response = H.lookup(k);
	while (!response.is_ready()) {}
	v = response.get_result();
\end{verbatim}
Such a radical change cannot be well-received in any computing community.

In conclusion, we cannot accept most design choices of DRAMHiT, but it is important to highlight their observation that main memory has become the limiting factor of concurrent hash table designs.
This problem is partially addressed in \texttt{AtomicDict}'s \texttt{{batch\_lookup}} and \texttt{reduce} operations.


\section{Completely Lock-Free Hash Tables}\label{sec:on-the-feasibility-of-completely-lock-free-hash-tables}

Most operations on Maier's hash table are lock-free.
This, crucially, does not comprise the migration operation as well.
That a fully lock-free migration process is possible has been shown by Hesselink et al.\ in~\cite[\S3.5]{hesselink}.
Notwithstanding the theoretical achievement, the practicality of such an approach can be subject to debate.

A migration process, as described in~\cite{hesselink}, comprises $\Omega(n)$ atomic memory writes.
An atomic write can be considered to be an order of magnitude more expensive than a normal write.
For instance, Intel SkyLake's \texttt{lock~cmpxchg8b} has a measured latency of 23 clock cycles, while a \texttt{mov} instruction from a register to memory has a latency of 2 cycles.
(Agner Fog provides excellent and comprehensive tables for individual instruction latencies over several x86 processor architectures in~\cite{x86-instruction-tables}.)

Because of the cost of atomic writes, it is easy to see how the cost of migrations quickly becomes prohibitive, as noted in~\cite{maier} as well:
\begin{quote}
	While [\ldots] lock-free dynamic linear probing hash tables are possible, there is no result on their practical feasibility.
	Our focus is geared more toward engineering the fastest migration possible; therefore, we are fine with small amounts of locking, as long as it improves the overall performance.
\end{quote}

The present author has also taken the same stance.
At the current time, it appears unfeasible to use completely lock-free hash tables in practice.
Furthermore, it is hard to believe that such limitations on the performance of atomic operations will ever be lifted by the hardware of the future.
