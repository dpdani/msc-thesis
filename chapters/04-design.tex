\chapter{Design}\label{ch:design}


\epigraph{
    There are two ways of constructing a software design: One way is to make it so simple that there are \emph{obviously} no deficiencies, and the other way is to make it so complicated that there are no \emph{obvious} deficiencies.
}{C.\ A.\ R.\ Hoare,\\The Emperor's Old Clothes.}


In this Chapter, we describe the design that we chose for \texttt{AtomicDict}, a hash table for the CPython interpreter.
It is a concurrent, linear-probing, resizable hash table, with double-hashing, and split index and data tables.
Linear probing is found to be effective on modern hardware due to its excellent cache behavior.
The resizing scheme is drawn from Maier's hash table, and the split index and data tables from CPython's design.
We additionally enhance this scheme with other useful features for a concurrent context: synchronous operations, reservation buffers, and iteration partitioning.
Finally, we include methods for batching operations so as to amortize the cost of memory accesses.

We were required to choose a double-hashing scheme because of the differences in hash computations in CPython and in Maier's hash table.
That is, if we did not re-hash CPython-generated hashes almost all keys would collide in the first slot, given that it is often false that the hash is a ``large pseudo-random number,'' as Maier's migration scheme requires.
(Recall Section~\ref{subsec:python-generated-hash-values}.)

Synchronous operations extend the obstructing nature of Maier's migration design for other operations: consistent size-retrival, and sequentially consistent iterations.
Adding other sequentially consistent operations in the future remains possible, based on this framework.
We therefore maintain like~\cite{maier}, but we extend to synchronous operations as well, that:
\begin{quote}
[\ldots]
    The hash table works lock-free as long as no [synchronous operation] is triggered.
\end{quote}

To drive our design choices, we hold the following claims, which we confirm later in Section~\ref{sec:dict-metrics}:
\begin{enumerate}
    \item the common usage of a hash table consists of 90\% lookups, 9\% insertions, and 1\% deletes; and
    \item most lookups fail.
\end{enumerate}

The first claim should not be surprising.
(See for instance~\cite[\S13.1]{art-mp}.)
The second one may be explained by the commonality of iterations; that is, of operations that return the entire key-value set.
Instead of looking up all the individual keys which \emph{are known} to be in the table, it may be that Python programs prefer to iterate over all keys.
And furthermore, it may be that when a program looks up one specific key, in most cases it really \emph{isn't known} whether the key is present or not.

Based on this claim, we enhance the table hashing with the Robin Hood scheme~\cite{robin-hood,bolt}, which is especially effective in pruning searches for keys which are not in the table.
Our approach, detailed later in Section~\ref{sec:lazy-robin-hood}, differs from~\cite{bolt} in that we don't necessarily constrain the hash table to this scheme: we maintain the Robin Hood invariants as long as we can do so in a lock-free manner.
When the maintenance of those invariants requires an increasing number of atomic operations to be carried out in the index, we instead fall back to regular linear probing.
Before actually inserting a key that does not maintain the Robin Hood invariants, we mark the hash table as not \emph{compact}.
Upon initialization, and after certain migrations, the hash table is marked as compact.
A compact operation can be requested by the user to recover the Robin Hood invariants.


\section{Split Index and Data Tables}\label{sec:python-dict}

The hash table itself is an index over another table, called the data table.
In the latter is where the pointers to the keys and values objects reside.
The index table is sparse, so as to cope with collisions and their resolution, and the data table is dense, so as to provide better performance for iterations.
Collision resolution only addresses the index.
We call \emph{slots} the elements of the index table, and \emph{entries} the elements of the data table.
Slots may be occupied by \emph{nodes}.

The size of a node depends on the capacity of the hash table, and it ranges from 1 to 8 bytes.
The node contains the location of the entry it points to, the distance from its distance-0 slot, and an optional tag.
Depending on the capacity of the hash table, the node and distance fields are rescaled as well, according to Table~\ref{tab:nodes}.

\begin{table}
    \centering\begin{tabular}{llll|llll}
                  $\log m$ & Node & Distance & Tag & $\log m$ & Node & Distance & Tag \\
                  \hline
                  6        & 8    & 2        & 0   & 32       & 64   & 5        & 27  \\
                  7        & 16   & 3        & 6   & 33       & 64   & 6        & 25  \\
                  8        & 16   & 3        & 5   & 34       & 64   & 6        & 24  \\
                  9        & 16   & 4        & 3   & 35       & 64   & 6        & 23  \\
                  10       & 16   & 4        & 2   & 36       & 64   & 6        & 22  \\
                  11       & 32   & 4        & 17  & 37       & 64   & 6        & 21  \\
                  12       & 32   & 4        & 16  & 38       & 64   & 6        & 20  \\
                  13       & 32   & 4        & 15  & 39       & 64   & 6        & 19  \\
                  14       & 32   & 4        & 14  & 40       & 64   & 6        & 18  \\
                  15       & 32   & 4        & 13  & 41       & 64   & 6        & 17  \\
                  16       & 32   & 4        & 12  & 42       & 64   & 6        & 16  \\
                  17       & 32   & 5        & 10  & 43       & 64   & 6        & 15  \\
                  18       & 32   & 5        & 9   & 44       & 64   & 6        & 14  \\
                  19       & 32   & 5        & 8   & 45       & 64   & 6        & 13  \\
                  20       & 32   & 5        & 7   & 46       & 64   & 6        & 12  \\
                  21       & 32   & 5        & 6   & 47       & 64   & 6        & 11  \\
                  22       & 32   & 5        & 5   & 48       & 64   & 6        & 10  \\
                  23       & 32   & 5        & 4   & 49       & 64   & 6        & 9   \\
                  24       & 32   & 5        & 3   & 50       & 64   & 6        & 8   \\
                  25       & 32   & 5        & 2   & 51       & 64   & 6        & 7   \\
                  26       & 64   & 5        & 33  & 52       & 64   & 6        & 6   \\
                  27       & 64   & 5        & 32  & 53       & 64   & 6        & 5   \\
                  28       & 64   & 5        & 31  & 54       & 64   & 6        & 4   \\
                  29       & 64   & 5        & 30  & 55       & 64   & 6        & 3   \\
                  30       & 64   & 5        & 29  & 56       & 64   & 6        & 2   \\
                  31 & 64 & 5 & 28 & \\
    \end{tabular}
    \caption{Sizes of internal fields of nodes in the index: $m$ is the capacity of the table, \emph{Node}, \emph{Distance}, and \emph{Tag} refer to their respective sizes, expressed in number of bits.}
    \label{tab:nodes}
\end{table}

The tag contains a portion of a key's hash.
When the tag is not the same as the equivalent portion of the hash of the key looked up, the relevant entry in the data table is not visited.
This reduces the cost of having split index and data tables.
That is, when a lookup visits the index, it reads the distance-0 slot of the searched key: a random address in memory.
Then, it linearly reads the nodes in a probe, which are likely to be in the same cache line of the distance-0 slot.
Each time a tag indicates that the node may refer to an entry containing the searched key, the entry must be read, at a different, random address in memory.
The tag serves to amortize the cost of the second, random memory read.


\section{Migrations}\label{sec:migrations-design}


Let us extend this concept by including other operations as well.
We will collectively refer to migrations and other kinds of operations that break the lock-freedom of the hash table as \emph{synchronous operations}, described later in Section~\ref{sec:synchronous-operations}.
The essential characteristic is that their implementation may be greatly simplified by not allowing concurrent lock-free access; both from the perspective of computational costs, and maintenance costs.

Though, given this fundamental difference in picking the hash bits to determine the position in the hash table, it was necessary to re-hash Python-generated hashes, or else the $d_0$ position of very many objects would be $0$.
In fact, the usage of the most-significant bits scheme is a core assumption for proving~\cite[Lemma~5.1]{maier}.
This is done with a cheap CRC32 hardware instruction, that Maier also used in his implementation.
Thus, the required property of hashes being ``\emph{large} pseudo-random numbers,'' is respected.

Maier et al.\ present different various strategies for a migration process to be carried out.
In this design a single choice, over the four possible variants is made.
With regards to the strategy for ``[hiding] the migration from the underlying application,'' two choices are presented: either ``[Recruit] User-Threads (u),'' or ``[Use] a Dedicated Thread Pool (p)''.
We find that it would be surprising for the programmer to find out that a thread pool gets created at each instantiation of a hash table, and that choice would also not be much more performant according to~\cite[\S8.4, Using Dedicated Growing Threads]{maier}.
With regards to the consistency semantics of migrations (that is, in spite of concurrent mutations), two choices are presented again: either ``[Mark] Moved Elements for Consistency (a--asynchronous),'' or ``[Prevent] Concurrent Updates to Ensure Consistency (s--semi-synchronized).''
We find that there is one clear choice again: the semi-synchronized strategy seems easier to implement (and to maintain), while also being more performant on average, according to Maier's own measurements.
Thus, in the following Section~\ref{subsec:quick-migrations}, we only describe what Maier refers to as the \emph{usGrow} implementation variant for migrations.


\section{Reservation Buffers}\label{sec:reservation-buffers}

The usage of pages for the data table, implicitly creates zones of contention.
When threads want to add a new key into the hash table, the most-recently added page is the one in which the insertion is attempted.
So that effectively all threads are trying to contend one page of the data table.

The degree to which contention is exhibited depends on the strategy with which threads decide which free entry in the page they wish to take for the new item.
Consider a strategy in which the lowest-numbered entry is always chosen: every thread always tries to reserve the same entry, with only one thread succeeding at any given time.

A simple, yet much better, alternative is to treat the page itself as a sort of hash table.
That is, instead of choosing an entry based on the current availability, an entry is chosen based on the hash of the key.
Thus, with sufficiently uniformly distributed hash values, the contention is greatly reduced.

The reservation itself needs to be carried out with atomic operations, so that the cost of inserting one key is always at least two atomic operations: one write to the data table, and one write to the index.
In order to amortize the cost of having a data table separate from the actual hash table (the index), instead of reserving one entry at every insertion, we let threads reserve a configurable number of entries, four per default.
The allowed values are 1, 2, 4, 8, 16, 32, or 64, with 64 being the size of an entire data table page.
When a thread has no available reservations, it resolves to finding a free entry based on the key's hash as described before, reserving four entries; instead when a thread has reservations at disposal, it directly writes into the free entries that it owns, using regular (non-atomic) writes.

In summary, the expected number of atomic writes per inserted key is $1 + 1/4 = 1.25$.


\section{Accessor Storage}\label{sec:accessor-storage}

The reservation buffer, along with other things, is stored inside an accessor-local storage.
It contains:
\begin{enumerate}
    \item a mutex;
    \item a local view of the size of the dictionary;
    \item a flag indicating whether this accessor (thread) is participating in the current migration; and
    \item the reservation buffer.
\end{enumerate}

The mutex itself protects the contents of the accessor storage.
This may seem counter-intuitive, but it is actually very useful.
First of all, when a dictionary needs to be freed, all the allocated accessor storages need to be freed as well.
In order to do this, the accessor storages are kept in a list.
A thread freeing the dictionary traverses the list to free the accessor storages, and a thread accessing the dictionary for the first time, appends its newly created accessor storage to the list.

The mutex itself is thus generally held only by its accessor, which releases it at the end of any insert, or delete operation.

Furthermore, when a thread becomes the leader of a migration (detailed later), it may necessitate to modify the reservation buffers of the other threads, in case the data table is modified.
E.g.\ during a shrink migration, the data table is shrunk, in addition to the index, in order to free unoccupied space.
That also entails that the entries in the reservation buffers need to be changed, because their location relative to the start of the data table has changed.


\section{Synchronous Operations}\label{sec:synchronous-operations}

In abstract terms, the presence of the list of thread-local mutexes, described in the above Section, enables the creation of cuts in the execution of dictionary operations, inasmuch as the acquisition of all the thread-local mutexes by one thread creates a distinction between the operations that happened before this circumstance and those that happened afterwards.

This crucial characteristic, which is required e.g.\ for the re-ordering of data table entries, enables also many more usages.
A few are described in the following sections.
In particular, it enables a very simple mechanism for ensuring that all accessors come to know the presence of a hash table migration, for establishing the correct size of the hash table, and for performing a sequentially consistent iteration of the items in the hash table.

All of these operations are called synchronous because they all share the common necessity to be carried out sequentially by one thread, or rather that at least one part of their execution needs to be performed sequentially.
For instance, the hash table migration enjoys the help of more than one thread, but requires a step in which the leader performs the necessary alterations to the data table before other threads can join in the migration.

The addition of this mechanism ensures that the dictionary presented here is capable both of performant lock-free operations, and of complex operations that require the exclusive access to the entire dictionary in order to be performed without a prohibitive number of expensive atomic memory writes.
The properties that a synchronous operation is sequential and ensured to be mutually exclusive w.r.t.\ all other threads, make it also very simple to be explained and understood, an important characteristic for collaborative development.

Do note that the presence of a synchronous operation does not obstruct concurrent lookup operations, as also described in the relevant later sections.
The meaning is that for concurrent migrations, the lookup linearizes on the state of the hash table prior to the migration (the thread performing the lookup will not be participating in the migration process).
For other arbitrary synchronous operations, in principle this behavior may not be acceptable; nevertheless, for those considered so far, it is indeed acceptable.
This exemption of lookups from the participation in migrations follows along the lines of~\cite[\S5.3.2, Preventing Concurrent Updates to Ensure Consistency]{maier}, i.e.\ only mutations are prevented, read-only operations are allowed to proceed on the prior state of the hash table.


\section{Lazy Robin-Hood}\label{sec:lazy-robin-hood}

The Robin Hood state for the hash table is essentially the distance each key has from its distance-0 position.
In other words, the number of collisions.

In order to keep track of this distance, we steal some bits from the nodes in the index.
According to~\cite[Corollary to Theorem~3]{robin-hood}, the expected number of collisions per probe for a full table is $\Theta(\log n)$, e.g.\ four for a table of size 64.
Such is a relatively low number, thus we can steal two bits, or in general $\log \log n$ bits, from the index nodes.

We do so, by following the values in Table~\ref{tab:nodes}.
As you can observe the proposed hash table has definite minimum and maximum sizes, respectively of $2^{6} = 64$ and $2^{56} \approxeq 7.2 \times 10^{16}$.


Comparing with~\cite{robin-hood}, we have slightly modified the invariant so as to favor avoiding moving elements on a tie.

The laziness in our approach is that if the \emph{aligned} size of the write required to safely insert the node into the index exceeds 128-bits, then we refrain from maintaining the Robin Hood invariants at all, for we would need to employ more than one atomic writes in order to store the newly inserted key.
This is because we cannot use the hardware~\cite[CMPXCHG--Compare and Exchange]{x86-64} instruction over more than 16-bytes.
The instruction further requires that the write be 16-byte aligned, or else a segmentation fault is encountered.
(Other faults may be encountered as well, depending on the architecture; for instance in \texttt{aarch64} ISAs a bus fault is encountered.
ISAs other than x86-64 are not currently supported.)
Thus, e.g.\ if a new node is inserted into the index after a 16-byte boundary, the insertion cannot be performed with a normal (i.e.\ compact) node.

That is, when the distance cannot be stored in $O(\log \log n)$ space, we semantically say the distance is $\infty$ and store it as $\log \log n - 1$ (we may later refer to this as the maximum distance for a given size of the table, or simply as the maximum distance).
When a node has distance equals to the maximum distance, we say that node is a non-compact node.
We also employ non-compact nodes to denote tombstones.
As is well known, see for instance~\cite[\S6.4]{the-art-vol-2}, ``the obvious way of deleting records from a hash table doesn't work.''
Instead of simply removing a node, we swap it for a tombstone node, which is a non-compact node pointing to entry number 0 of the data table.
The data table maintains the invariant that the key and value fields of that entry are always \texttt{NULL}.
This is achieved by requiring at initialization time that the thread creating the \texttt{AtomicDict} instance reserves entry number 0 for itself, and marks it as already occupied, without writing anything into it.
Thus, all data tables have at least one unused entry.
Furthermore, if the thread that created the hash table never writes into it, there will always be $|R|$ unused entries, with $|R|$ being the size of the reservation buffer.
For more details on reservations see \S\ref{sec:reservation-buffers}.
Operations that read the index, simply always skip non-compact nodes that point to entry 0.
Further details on tombstone nodes and deletes are given in \S\ref{sec:delete}.

When a new node could be inserted, respecting the requirements for our lazy robin hood, but it should be inserted at a distance greater then the maximum, we initiate a grow migration, as described in~\S\ref{sec:migrations}.
In effect, we have a local trigger for our migrations that doesn't require global knowledge of the table.
We can observe the local effect of collisions to determine that the table size is too small, and take action accordingly.
This is not the only trigger for migrations, for a complete list refer to the relevant section.
One may argue that a degenerate set of hashes can continuously trigger migrations.
While this is true in principle, do observe an implication of Table~\ref{tab:nodes}: starting from a table capacity of $2^9 = 512$, the maximum distance is 16, while the number of nodes in a 16-byte-aligned region of memory (a quad word), is 8.
Therefore, the described mechanism effectively cannot be triggered indefinitely.


\section{Garbage Collection}\label{sec:garbage-collection}

As Micheal Maged put it in~\cite[\S2.3]{micheal-hash-tables}:
\begin{quote}
[\ldots]
    The failure or delay of the garbage collector may prevent threads operating on lock-free objects from making progress indefinitely, thus violating lock-freedom.
\end{quote}

Such is undeniably correct, and therefore, the proposed implementation for \texttt{AtomicDict} cannot be considered lock-free, simply by virtue of the fact that its memory is managed through Python's garbage collector.
Notwithstanding, this was a deliberate implementation detail and not a foundational piece in the design of \texttt{AtomicDict}.
The fact that its internal data structures are traced with Python's GC is something that can be easily changed without compromising the design.
The choice was chiefly made to simplify the implementation of \texttt{AtomicDict}.
Furthermore, given the fact that the hash table access was subject to the interpretation of Python code by the CPython interpreter, as that is the target for the presented work, there was no possibility of not having to deal with a stop-the-world GC\@.

Python code is not the only means through which \texttt{AtomicDict}'s APIs can be accessed.
The library also exposes C header files (though, their standalone usage has not yet been explored).
For those use-cases that don't employ Python code, but rather directly call \texttt{AtomicDict}'s C functions, true lock-freedom can be of interest.

Changing the implementation to not be based on the provided GC, but instead using a dedicated memory management system, is entirely possible; albeit requiring a lot of additional work.
One way this can be achieved is by implementing what Micheal himself describes in~\cite{micheal-safe-reclamation}, which employs so-called \emph{hazard pointers} (or any other suitable algorithm).

Serious effort was not put into looking at this possible implementation change, as of the time of writing.
