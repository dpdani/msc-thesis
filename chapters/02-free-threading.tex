\chapter{Free-Threading Python}\label{ch:free-threading-python}

In this Chapter we delve into the modifications necessary to enable free-thread\-ing in CPython.
This is a combination of several different optimizations primarily focused on maintaining single-threaded performance, in a concurrent context.

We start by examining concurrent, biased reference counting, which optimizes performance by assuming most objects are accessed primarily by the thread that created them.
Deferred reference counting is introduced for objects where consistent reference counting is impractical.
These objects defer computing their true reference count until Garbage Collection (GC) pauses.

Quiescent State-Based Reclamation (QSBR) is presented as a mechanism to support lock-free read accesses to built-in data structures, optimizing those accesses which are the most frequent.
QSBR delays freeing objects until no thread can be in the process of accessing them, preventing use-after-free bugs.
To support this change, it was also necessary to change CPython's memory allocator to mimalloc, a thread-safe alternative.
Adjustments were also necessary for CPython's GC, which so becomes a ``stop-the-world'' collector.

Finally, we also detail the thread-safety of built-in data structures.
Per-object mutexes maintain the atomic operations previously ensured by the GIL, similarly to Java's model, with some read-only operations optimized to avoid locking, relying on some modifications to mimalloc for consistency.


\section{Concurrent, biased reference counting}\label{sec:concurrent-biased-reference-counting}

Concurrent reference counting is an implementation of reference counting that maintains correctness in multithreaded environments.
Free-threading Python implements a design of concurrent reference counting based on the assumption that most objects are only accessed by the thread that created them.
This bias towards the owner's reference counting does in fact turn out to be justified.
This design is backed by the relevant literature~\cite{biased-refcounting}.
Overall, the result of biased reference counting is that the impact of concurrent reference counting is greatly reduced; that is, most reference counting operations use the biased, faster path.

An object is modified to store an owner thread identifier, an owner's reference count, and a shared reference count.
The owner thread may use normal reads and writes when modifying its reference count, while the other threads must use atomic operations to modify the shared reference count, thus resulting in a slower path for reference counting.

The shared reference count also stores the reference counting state, using two bits of the 4-byte counter.
An object may be in one of the following states:
\begin{enumerate}
    \item default;
    \item weakrefs;
    \item queued; or
    \item merged.
\end{enumerate}
Their meanings are described in~\cite[\S Biased Reference Counting]{pep703}:
\begin{quote}
    The states form a progression: during their lifecycle, objects may transition to any numerically higher state.
    Objects can only be deallocated from the ``default'' and ``merged'' states.
    Other states must transition to the ``merged'' state before deallocation.
    Transitioning states requires an atomic compare-and-swap on the [shared reference count] field.
\end{quote}

\paragraph{The ``default'' state.}
Objects are initially created in this state.
(Logically, an object cannot be shared among multiple threads before and during its instantiation.)
If an object, during its lifetime, was in fact only accessed by its owner, it will have remained in the ``default'' state, and can thus enjoy the quick deallocation path: upon reaching a reference count of 0, the object's memory is immediately freed (this is what happens to Python objects in the default build of CPython).

\paragraph{The ``weakrefs,'' ``queued,'' and ``merged'' states.}
Objects that are actually shared between threads during their lifecycle cannot use the quick deallocation path, as~\cite[]{pep703} states:
\begin{quote}
    [\ldots] The first time a non-owning thread attempts to retrieve an object in the ``default'' state it falls back to the slower locking code path and transitions the object to the ``weakrefs'' state.
\end{quote}

An object enters the ``queued'' state when a non-owning thread wishes to merge the two reference counts; that is, when the shared reference count becomes negative.
The object is then enqueued in the owner's queue of objects to be merged.
When the object enters the ``merged'' state, its owner thread field is reset, indicating that the object is not owned by any thread, and the owner's reference count is no longer used.
When the shared reference count reaches 0, the object may be deallocated, when a quiescent state is reached (see Section~\ref{sec:qsbr}).

Note that these transitions rarely happen, because (1) most objects are accessed by a single thread, and (2) it is rare for an object to have a negative shared reference count.


\paragraph{Thread identifiers.}
It logically follows from the above discussion that threads accessing CPython objects must be uniquely identified.
Such identifiers are computed by calling CPython's new \texttt{\_Py\_ThreadId} internal API\@.
Internally, it uses various hardware- and platform-dependent calls to generate the number.
For instance, on x86-64 hardware running Linux, the identifier is stored in the FS register.
It is a pointer to the thread's \texttt{pthread} struct, used primarily for fast access to its Thread Local Storage (TLS).
Refer also to the Linux Kernel Documentation, \S29.8.1, Common FS and GS usage.
Available online at \url{https://www.kernel.org/doc/html/v6.9/arch/x86/x86_64/fsgs.html}.\footnote{Last accessed \today.}
This API is also used in this thesis to elect the leader thread for a hash table migration.
(See Section~\ref{subsec:migration-leader}.)


\section{Deferred reference counting}\label{sec:deferred-reference-counting}

For some particular objects, it does not make sense to keep a consistent reference count.
The owner's reference count also stores two flags, used to indicate that the object is immortal, or that it can use deferred reference counting.
Immortal objects are objects whose lifetime equals the lifetime of the program, such as the \texttt{True}, \texttt{False}, and \texttt{None} objects.

Reference counts for some non-immortalized, contended objects may be deferred.
Namely, for ``top-level functions, code objects, modules, and methods,'' because ``these objects don't necessarily live for the lifetime of the program, so immortalization is not a good fit.''

In~\cite[\S Deferred Reference Counting]{pep703} Gross details an approach where instead of letting the interpreter increment and decrement those objects' reference counts as they are pushed to and popped from the evaluation stack, it instead skips those reference counting operations.
It follows that their ``reference count fields no longer reflect their true number of references.''
Instead, their true reference counts can only be safely computed during Garbage Collection (GC) pauses, and thus such objects can only be deallocated during a GC cycle.
CPython's GC was designed to only collect objects that are part of reference cycles.
It has thus been modified in order to also compute true deferred reference counts, and to collect those objects as well.

This is not a general approach to deferred reference counting, and thus departs from the existing literature; cf.~\cite{deferred-refcounting}.


\section{Quiescent State-Based Reclamation}\label{sec:qsbr}

An additional change to CPython for free-threading is QSBR\@.
Although not directly referred to as QSBR, it was initially described in~\cite[\S Mimalloc Page Reuse]{pep703}, and later discussed in greater detail in the accompanying GitHub issue~\cite{qsbr}.
CPython's implementation of QSBR closely follows FreeBSD's implementation of Globally Unbounded Sequences, as noted by Gross in the referred issue.

This mechanism serves to support lock-free read accesses to built-in data structures.
For example, when a thread $t_r$ optimistically avoids locking a shared \texttt{dict} $H$'s keys array, due to a read-only access, an unsafe circumstance may arise.
If $H$ is resized (migrated) by another thread $t_m$ while $t_r$ is operating its read, it may be that $t_m$ frees $H$'s keys object before $t_r$ gets a chance to read it.
This circumstance may arise while $H$'s reference count remains $> 0$, thus this problem does not pertain concurrent reference counting.

In the above example, $t_m$ is not allowed to free the keys immediately, instead the following happens:
\begin{enumerate}
    \item there exist a global, monotonically increasing sequence number $S$, accessible to every thread $t \in T$;
    \item $t_r$ is endowed with a local sequence number $S(t_r)$, which records the most recently observed value of the global $S$, say that in this execution $S(t_r)~=~S$;
    \item $t_r$ begins the read-only access over $H$ (until the access ends, $t_r$ is not allowed to change $S(t_r)$);
    \item $t_m$ begins the resize (migration) of $H$;
    \item $t_r$ and $t_m$ read the keys object $k$ (a pointer) associated with $H$;
    \item $t_m$ creates a new keys object $k'$, and completes the resizing (migration of keys in $k$ to $k'$);
    \item $t_m$ substitutes the key object associated with $H$, from $k$ to $k'$;
    \item $t_m$ calls for a \emph{delayed} free of $k$;
    \item $t_m$ observes the value of $S$;
    \item $t_m$ adds a pointer to $k$ in a shared QSBR queue $Q$, and sets the QSBR goal $g$ for its deferred free to $g(k) = S + 1$;
    \item $t_r$ completes its read;
    \item $t_r$ increments its local sequence number, $S'(t_r) = S(t_r) + 1 = S + 1$;
    \item \ldots\emph{eventually}\ldots
    \item a thread $t_q$ (it may be that $t_q \equiv t_r \vee t_m \equiv t_m$) is in a quiescent state, because it is running a GC cycle;
    \item $t_q$ observes that there exist a lower bound $\sigma$ to the sequence number of every thread $\min_{t \in T} S(t) \geq \sigma$;
    \item if $g(k) > \sigma$, $k$ will not be freed;
    \item otherwise, $k$ will be freed, and removed from $Q$.
\end{enumerate}

This is a generic mechanism that can help implement lock-free data structures.
Note that thread-local (i.e.\ not shared) data structures are not affected by QSBR, and are instead immediately freed upon reaching a reference count of 0, because they have remained in the ``default'' state.


\section{Changes to the memory allocator}\label{sec:mimalloc}

The described QSBR mechanism was located in the ``Mimalloc Page Reuse'' section of PEP~703 because it requires changes to CPython's memory allocator in order to work.
In fact, the memory allocator employed so far in CPython, is not thread-safe.
To allow allocations of objects without locking the entire interpreter, it was decided to change memory allocator entirely.

The new allocator of choice is mimalloc, a general purpose allocator with good performance and thread-safety.
It is described in detail in~\cite{mimalloc}.

Some changes to mimalloc were necessary in order to support QSBR, essentially around restricting page reuse.
When a mimalloc ``page'' (not equiv.\ to an OS memory page) is empty, mimalloc may decide to reuse it for incoming allocation requests.
Though, it may be that an empty page contains objects whose free operation was deferred, due to QSBR\@.
Therefore, it would be unsafe to reuse the page for new objects.
Instead, the following happens:
\begin{enumerate}
    \item a thread deletes an object, which happened to reside in mimalloc page $p$;
    \item $p$ is now empty;
    \item $p$ gets tagged with the global sequence number $S$ (described earlier in Section~\ref{sec:qsbr}), s.t.\ $\tau(p)~=~S$;
    \item $p$ may be reused when a later global sequence number $S' > \tau(p)$ is observed.
\end{enumerate}

Crucially, this modification to mimalloc ensures that threads which may still read an object in $p$ don't read unrelated objects, resulting in undefined behavior, and instead necessarily read a reference count of 0, because the freed object is still stored in the same mimalloc page, which is not reused until a quiescent state is reached.


\section{Garbage collection}\label{sec:python-gc}

CPython's GC was also changed for the free-threading build to work.
Foremost, it was important to run the GC during ``stop-the-world'' pauses, to restore thread-safety guarantees that were previously provided by the GIL\@.
Additionally, it was also necessary to integrate it with deferred and biased reference counting.

To support ``stop-the-world'' pauses a status is assigned to each thread; one of ``attached,'' ``detached,'' or ``gc''.
The ``attached'' and ``detached'' states follow the semantics that were previously assigned to the acquisition and release of the GIL\@.
A thread that would previously acquire the GIL, transitions to the ``attached'' state; conversely, a thread that would previously release the GIL, transitions to the ``detached'' state.

When a GC cycle begins, the GC thread transitions all other threads that were in the ``detached'' state into the ``gc'' state.
Threads in the ``attached'' state are requested to pause themselves and transition to the ``gc'' state.
When all threads are in the ``gc'' state, the GC thread starts the collection cycle.
Once the collection finishes, it transitions all threads in the ``gc'' state to the ``detached'' state, and notifies them.
If a thread was in the ``detached'' state before entering the ``gc'' state, it ignores the notification.

Note that ``stop-the-world'' pauses also implicitly create quiescent states (when all threads are in the ``gc'' state).
The deterministic presence of quiescent states is required for QSBR to function properly.


\section{Thread-safety of built-in data structures}\label{sec:thread-safety-of-builtin-data-structures}

Most operations in Python are not atomic, but historically some were in fact made implicitly atomic due to the presence of the GIL\@.
For instance, calling \texttt{{list(set)}} atomically creates a list from a set, because the GIL is held for the duration of the call.
The thread safety guarantees of built-in objects that were implicitly provided by the GIL, are retained, as detailed in~\cite[\S Container Thread-Safety]{pep703}.

Note that this does not mean that all operations on built-in data structures are thread safe.
(In fact, an operation like \texttt{{my\_dict[key] += 1}} cannot be made thread-safe, in the same way a C instruction \texttt{{my\_counter++}} cannot be made thread-safe, without external synchronization.)
Instead, Gross proposes to simply preserve the guarantees that were in place before the GIL was removed.

To achieve this goal, CPython objects are endowed with per-object mutexes.
When an operation that was previously thread-safe because of the GIL is requested, the object involved is locked and the operation performed.
Some operations require that two objects are locked, e.g.\ \texttt{{list(set)}}.
This is done by locking first the object whose memory address is lower, and then the other, to avoid deadlocks.

\paragraph{Optimistically avoiding locking.}
To improve performance, a few operations have been selected to be executed without any locking.
They are frequently used, read-only operations on \texttt{list} and \texttt{dict} objects (e.g.\ \texttt{{my\_list[0]}}).
Such operations may fall back to a slower path which acquires the involved objects' mutexes, in case the container objects were concurrently modified during the read operation.
(This optimistic fast-path requires the changes to mimalloc described in Section~\ref{sec:mimalloc}.
See also~\cite[\S Optimistically Avoiding Locking]{pep703}, for further details.)
