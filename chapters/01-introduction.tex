\chapter{Introduction}\label{ch:introduction}

The focus of this Thesis is the development of an efficient concurrent hash table that can be used directly from Python code.
Given the ubiquitous use of hash tables in Python programs, the research aims to contribute to the Python community by proposing a data structure that meets the performance and scalability demands of a post-GIL Python environment.

This Thesis is organized into several chapters, beginning with an introduction to hash tables, concurrent hash tables, and their applications, followed by an exploration of the challenges and solutions related to free-threading CPython.
We then review the state of the art in concurrent hash table design, detail the design and implementation of the proposed data structure, and conclude with an evaluation of its performance.

Overall, this work provides a significant contribution to the field of concurrent data structures for Python and offers practical insights for its community as it prepares for new possibilities in parallel computing with the upcoming changes to CPython.

This Chapter begins by recalling hash tables and their many applications in computing.
It then delves into the relevance of concurrent hash tables.
The Chapter also touches on CPython's built-in hash table, \texttt{dict}, which is widely used within the CPython interpreter and in the outer Python ecosystem.
The final Section introduces ``free-threading Python,'' an experimental build of CPython that removes the GIL in the upcoming 3.13 release.


\section{Hash tables and their applications}\label{sec:hash-tables}

A hash table is a data structure that stores a set of distinct elements, called \emph{keys}, and associates with each of them a \emph{value}.
A \emph{{hash function}} $h(k)$ is used to determine a unique position for each key $k$ in an array of \emph{buckets} (or \emph{slots}), in which the desired key-value association can be found.
An \emph{ideal} hash function assigns each key to a unique slot, but such functions are very hard to find.
The occurrence of multiple keys having the same hash is called a \emph{collision}.
The mechanism with which a hash table handles collisions is called \emph{{collision resolution}}.

Hash tables support common operations, which are executed in expected constant time:
\begin{enumerate}
    \item an \emph{insert} creates a new (non previously existing) association of a key to a value;
    \item an \emph{update} replaces the value associated with a certain key with a new value;
    \item a \emph{delete} removes a key-value association; and
    \item a \emph{lookup} returns the value associated with a certain key; in particular, a lookup is described as \emph{successful} if it in fact returns said value, or alternatively it is described as \emph{failing}.
\end{enumerate}

Implementations of hash tables are generally distinguished by their choice of hash function and of collision resolution.
Common implementations of hash tables include:
\begin{itemize}
    \item open addressing, where items are placed into an array of slots, and collisions are resolved by displacing an item at slot $x$ to the next available slot $f(x)$, which may be computed by:
    \begin{itemize}
        \item linear probing, i.e.\ $f$ is a linear function;
        \item double hashing, i.e.\ $f$ is another hash function, distinct from $h$; or
    \end{itemize}
    \item separate chaining, also known as closed addressing, where items are placed into linked lists called buckets and collisions are resolved by appending to the buckets.
\end{itemize}

\paragraph{Applications of Hash Tables.}\label{par:applications-of-hash-tables}

Hash tables are ubiquitous in computing.
They are taught in entry level Computer Science courses, and form the basis of many programs.
Common applications include: map-reduce systems, databases (notably, their indexing and aggregation capabilities), caching systems, and dynamic programming, among others.
A characteristic that is common to these applications is that hash table access can dominate their execution time.

\subsection{Concurrent Hash Tables}\label{subsec:concurrent-hash-tables}

In a shared-memory concurrency environment, several threads can share information in a flexible and efficient way by sharing access to a common hash table.
In fact, hash tables can provide a form of \emph{natural parallelism}, as worded in the introduction to~\cite[Chapter~13]{art-mp}:
\begin{quote}
[\ldots]
    We studied how to extract parallelism from data structures [\ldots] that seemed to provide few opportunities for parallelism.
    In this chapter we take the opposite approach.
    We study \emph{{concurrent hashing}}, a problem that seems to be ``naturally parallelizable'' or, using a more technical term, \emph{{disjoint--access--parallel}}, meaning that concurrent method calls are likely to access disjoint locations, implying that there is little need for synchronization.
\end{quote}
In other words, two threads accessing a shared hash table have little or no need for synchronization when accessing two distinct keys stored therein.

In the following paragraphs of this Section, we provide some informal definitions for important properties of concurrent algorithms and data structures, drawing them from~\cite[]{art-mp}.

\paragraph{Linearizability.}
It is an important property of concurrent data structures: a method call carried out by a thread $t$, should be observable by any thread other than $t$, to have modified the state of the data structure instantaneously, at some moment between its invocation and response.

\paragraph{Lock-freedom and wait-freedom.}
\begin{quote}
    A method is \emph{lock-free} if it guarantees that infinitely often \emph{some} method call finishes in a finite number of steps.
\end{quote}
Wait-freedom is a more stringent property:
\begin{quote}
    A method is \emph{wait-free} if it guarantees that \emph{every} call terminates in a finite number of steps.
\end{quote}

These two are very desirable properties of concurrent data structures: they imply that threads don't need to wait for any other thread's work to complete, before completing their own work (wait-freedom), or that at least one thread can always make progress (lock-freedom).

In terms of Amdahl's Law, the implication is that the sequential part of a program is very small, so that the speedup from parallelization scales almost linearly with the number of threads.
That is, for $S$ being the speedup gained by parallelizing a program, $p$ being the parallelizable fraction of the program, and $n$ being the number of parallel processors:
\[
    S = \frac{1}{1 - p + \frac{p}{n}}, \text{~with~desired~} (1 - p) \ll p.
\]

The hash table design proposed in this Thesis is almost entirely lock-free.
Some well-defined relaxations of the lock-freedom property are taken in this design, and elaborated in Section~\ref{sec:migrations-design}.
These relaxations are not a novel contribution to the field of concurrent hash tables.


\section{Python and its Built-in Hash Table}\label{sec:dict-intro}

Python is a popular, high-level programming language.
It is dynamically typed and garbage-collected, and it chiefly supports object-oriented programming.

The CPython interpreter is the reference implementation of the Python language, and it is often confused for the language itself.
Other implementations of Python exist, but are less popular.
They are often specialized implementations that improve the performance of certain workloads.

Python has a built-in hash table, named \texttt{dict}, which is considered an integral part of the language.
It is a standard type, used in virtually every Python program, either explicitly, by directly creating a \texttt{dict} instance, or implicitly.
The CPython interpreter may create dictionaries in order to create an instance of a class, a very frequent operation in an object-oriented language.
The attributes of an object are entirely stored in a hash table, which can also be accessed directly.
In fact, the set of attributes of a Python object may be altered at runtime, thus differing from the one specified in its class.
(This was true up to version 3.12, released October 2023, where instances were optimized to avoid allocating this attributes dictionary.
The dictionary may still be created to view all the attributes of an object, upon a program's request.)

An exploration on the usages of the built-in hash table has been carried out and it is presented in Section~\ref{sec:dict-metrics}.
For a discussion on \texttt{dict}'s design, refer to Section~\ref{sec:python-dict-review}.
For a more thorough explanation of the common usages of CPython's \texttt{dict}, refer to~\cite[\S Principal Use Cases for Dictionaries]{dict-notes}.


\section{Free-threading Python}\label{sec:free-threading}

A known limitation of the CPython interpreter (not the Python language) is the presence of a Global Interpreter Lock (GIL).
This lock is used to protect the interpreter's internal state from concurrent mutations.
It has simplified significantly the implementation of the CPython interpreter over the years, at the cost of Python programs being often unable to efficiently use the multiple processors available in the system.
The limitation does not affect \emph{all} CPython-interpreted programs, in the sense that several programming paradigms do not make use of multi-threading regardless of the GIL, and are thus not actually impacted by its presence.

Examples of these paradigms, except from obviously single-threaded programs, include:
\begin{enumerate}
    \item co-routines-based concurrency, where a set of co-routines are executed concurrently and scheduled by the interpreter instead of the OS; and
    \item multiprocessing-based concurrency, where the separate strands of execution are offloaded from a main process to a set of child processes, each having their own GIL\@.
\end{enumerate}

Many other programs, instead, cannot effectively cope with the presence of the GIL\@.
Various times in the past, GIL removals have been attempted: see~\cite{dabeaz-gil}, and~\cite[\S Related Work]{pep703}.
The rationale behind them is always a variation of the same theme, quoting from~\cite{pep703}:
\begin{quote}
    CPython's [GIL] prevents multiple threads from executing Python code at the same time.
    The GIL is an obstacle to using multicore CPUs from Python efficiently.
    [\ldots] For scientific computing tasks, this lack of concurrency is often a bigger issue than speed of executing Python code, since most of the processor cycles are spent in optimized CPU or GPU kernels.
    The GIL introduces a global bottleneck that can prevent other threads from making progress if they call any Python code.
\end{quote}

It should be noted at this point that the addition of the GIL predates the popularity of multiprocessor systems.
(Although not yet called ``GIL,'' references to an interpreter lock can be found in source distributions of Python 1.0.1, released January 1994; available online at \url{https://www.python.org/ftp/python/src/python1.0.1.tar.gz}.\footnote{Last accessed \today.})
At the time, it made a lot more sense to simplify the implementation of the interpreter (and still support multi-threading), rather than being capable of more performance, but almost never enjoy it.

Free-threading Python (formerly known as nogil) is the latest attempt at removing the GIL\@.
Initially carried out by Sam Gross, it is now in CPython's official codebase.
Python 3.13, the next release, is scheduled for October 2024 (the year of writing), and will feature an experimental build mode for free-threading Python.
The build of Python 3.13 that will be commonly distributed, is referenced to as the \emph{default} build.

The work presented later in this Thesis is based on the original reference implementation, nogil, itself based on Python 3.9.
This choice was made because free-threading Python had not been implemented when the work for this thesis began in October 2023.
Porting this work to Python 3.13 is planned.

In the next Chapter, Free-Threading Python is described in more detail.
The changes described in this Thesis seem to be fully implemented, with the remaining work for the next release being primarily focused on resolving numerous minor issues.

Overall, free-threading is anticipated to result in a 6\% performance overhead for single-threaded executions and an 8\% overhead for multithreaded executions, with the higher overhead primarily due to the absence of optimizations available in single-threaded executions.
(See the article on Gross' presentation at the 2023 Python Language Summit~\cite{python-summit-2023-nogil}.)
