\chapter{Proposed Design and Implementation}\label{ch:design-and-implementation}

The implementation is available online at~\cite[src/cereggii/atomic\_dict]{cereggii}, and comprises 12 C source files, with Python bindings, and 29 test cases (written in Python code).

\cite{peniocereus-greggii}

$O(t)$

$\textrm{atw} \textrm{rgw} \textrm{rgr}$

The bindings to the Python runtime are considered an implementation detail, and are not discussed here.
They chiefly consist of mapping Python objects into C constructs, which is achieved with calls to Python's C-APIs.
Another feature of Python's C-APIs that has been extensively used is garbage collection, which is discussed in \S\ref{subsec:garbage-collection}.


\section{Design Overview}\label{sec:design-overview}


\subsection{Python's Dictionary}\label{subsec:python-dict}

A lot of the basis for \texttt{AtomicDict} has been inspired from Python's \texttt{dict}.
Namely, that the hash table itself is instead an index over another table, called the data table.
In the latter is where the pointers to the keys and values objects reside.

As described in~\cite[\S13.1]{art-mp}, we should be inclined to think that the usage of the dictionary consists of 90\% lookups, 9\% insertions, and 1\% deletes.
In \S\ref{sec:dict-metrics} we confirm this simple rule-of-thumb.
We further expose a counter-intuitive notion: most lookups fail.
(See Section~\ref{sec:dict-metrics} on \texttt{\_Py\_dict\_lookup} for the full details.)

This may be explained by the presence of iterations, that is, of operations that return the entire key-value set.
Thus, it may be that in most cases, when a program looks up one specific key in the dictionary, it really isn't known whether the key is there or not.
And furthermore, instead of looking up all the individual keys which are known to be there, it may be that programs prefer to iterate over all keys.
This would result in individual lookups exhibiting a tendency to fail.

Based on this, we enhance the table hashing with the Robin Hood scheme, known in the literature~\cite{robin-hood,bolt}, which is especially effective in pruning searches for keys which are not in the table.
Our approach, detailed in the following \S\ref{subsec:lazy-robin-hood}, differs from~\cite{bolt} in that we don't necessarily constrain the dictionary to this scheme: we maintain the Robin Hood invariants, described later, as long as we can do so in a lock-free manner.
When the maintenance of those invariants requires an increasing number of atomic operations to be carried out in the index, we instead fall back to regular linear probing.
When doing so, before actually inserting a key that does not maintain the Robin Hood invariants, we mark the hash table as not anymore \emph{compact}.
A compact operation can be requested by the user to recover the Robin Hood invariants.
This operation essentially consists of enlarging the hash table so that the invariants can be maintained.

We also optionally store tags in the nodes, further stealing some bits, in order to reduce the necessity of lookups into the data table.
The tag contains a portion of the hash of the key in the data table, when the tag does is not the same as the equivalent portion of the hash of the key looked up, the relevant entry in the data table is not visited.
This reduces the cost of having split index and data tables.


\subsection{Migrations}\label{subsec:maier}

Another substantial source of inspiration for \texttt{AtomicDict} is to be found in~\cite{maier}.
The core of the migration routine described later in~\S\ref{sec:migrations} is essentially the same as the one described in~\cite[\S5.3]{maier}.

We therefore maintain, like~\cite{maier}, that:

\begin{quote}
	[\ldots] The hash table works lock-free as long as no migration is triggered.
	% Once a migration is triggered, it is not lock-free but it happens asynchronously and it is hidden from any user of the hash table.
\end{quote}

Let us extend this concept by including other operations as well.
We will collectively refer to migrations and other kinds of operations that break the lock-freedom of the hash table as \emph{synchronous operations}, described later in Section~\ref{subsec:synchronous-operations}.
The essential characteristic is that their implementation may be greatly simplified by not allowing concurrent lock-free access; both from the perspective of computational costs, and maintenance costs.

Though, given this fundamental difference in picking the hash bits to determine the position in the hash table, it was necessary to re-hash Python-generated hashes, or else the $d_0$ position of very many objects would be $0$.
In fact, the usage of the most-significant bits scheme is a core assumption for proving~\cite[Lemma~5.1]{maier}.
This is done with a cheap CRC32 hardware instruction, that Maier also used in his implementation.
Thus, the required property of hashes being ``\emph{large} pseudo-random numbers,'' is respected.

Maier et al.\ present different various strategies for a migration process to be carried out.
In this design a single choice, over the four possible variants is made.
With regards to the strategy for ``[hiding] the migration from the underlying application,'' two choices are presented: either ``[Recruit] User-Threads (u),'' or ``[Use] a Dedicated Thread Pool (p)''.
We find that it would be surprising for the programmer to find out that a thread pool gets created at each instantiation of a hash table, and that choice would also not be much more performant according to~\cite[\S8.4, Using Dedicated Growing Threads]{maier}.
With regards to the consistency semantics of migrations (that is, in spite of concurrent mutations), two choices are presented again: either ``[Mark] Moved Elements for Consistency (a--asynchronous),'' or ``[Prevent] Concurrent Updates to Ensure Consistency (s--semi-synchronized).''
We find that there is one clear choice again: the semi-synchronized strategy seems easier to implement (and to maintain), while also being more performant on average, according to Maier's own measurements.
Thus, in the following Section~\ref{subsec:quick-migrations}, we only describe what Maier refers to as the \emph{usGrow} implementation variant for migrations.


\subsection{Reservation Buffers}\label{subsec:reservation-buffers}

The usage of pages for the data table, implicitly creates zones of contention.
When threads want to add a new key into the hash table, the most-recently added page is the one in which the insertion is attempted.
So that effectively all threads are trying to contend one page of the data table.

The degree to which contention is exhibited depends on the strategy with which threads decide which free entry in the page they wish to take for the new item.
Consider a strategy in which the lowest-numbered entry is always chosen: every thread always tries to reserve the same entry, with only one thread succeeding at any given time.

A simple, yet much better, alternative is to treat the page itself as a sort of hash table.
That is, instead of choosing an entry based on the current availability, an entry is chosen based on the hash of the key.
Thus, with sufficiently uniformly distributed hash values, the contention is greatly reduced.

The reservation itself needs to be carried out with atomic operations, so that the cost of inserting one key is always at least two atomic operations: one write to the data table, and one write to the index.
In order to amortize the cost of having a data table separate from the actual hash table (the index), instead of reserving one entry at every insertion, we let threads reserve a configurable number of entries, four per default.
The allowed values are 1, 2, 4, 8, 16, 32, or 64, with 64 being the size of an entire data table page.
When a thread has no available reservations, it resolves to finding a free entry based on the key's hash as described before, reserving four entries; instead when a thread has reservations at disposal, it directly writes into the free entries that it owns, using regular (non-atomic) writes.

In summary, the expected number of atomic writes per inserted key is $1 + 1/4 = 1.25$.


\subsection{Accessor Storage}\label{subsec:accessor-storage}

The reservation buffer, along with other things, is stored inside an accessor-local storage.
It contains:
\begin{enumerate}
	\item a mutex;
	\item a local view of the size of the dictionary;
	\item a flag indicating whether this accessor (thread) is participating in the current migration; and
	\item the reservation buffer.
\end{enumerate}

The mutex itself protects the contents of the accessor storage.
This may seem counter-intuitive, but it is actually very useful.
First of all, when a dictionary needs to be freed, all the allocated accessor storages need to be freed as well.
In order to do this, the accessor storages are kept in a list.
A thread freeing the dictionary traverses the list to free the accessor storages, and a thread accessing the dictionary for the first time, appends its newly created accessor storage to the list.

The mutex itself is thus generally held only by its accessor, which releases it at the end of any insert, or delete operation.

Furthermore, when a thread becomes the leader of a migration (detailed later), it may necessitate to modify the reservation buffers of the other threads, in case the data table is modified.
E.g.\ during a shrink migration, the data table is shrunk, in addition to the index, in order to free unoccupied space.
That also entails that the entries in the reservation buffers need to be changed, because their location relative to the start of the data table has changed.


\subsection{Synchronous Operations}\label{subsec:synchronous-operations}

In abstract terms, the presence of the list of thread-local mutexes, described in the above Section, enables the creation of cuts in the execution of dictionary operations, inasmuch as the acquisition of all the thread-local mutexes by one thread creates a distinction between the operations that happened before this circumstance and those that happened afterwards.

This crucial characteristic, which is required e.g.\ for the re-ordering of data table entries, enables also many more usages.
A few are described in the following sections.
In particular, it enables a very simple mechanism for ensuring that all accessors come to know the presence of a hash table migration, for establishing the correct size of the hash table, and for performing a sequentially consistent iteration of the items in the hash table.

All of these operations are called synchronous because they all share the common necessity to be carried out sequentially by one thread, or rather that at least one part of their execution needs to be performed sequentially.
For instance, the hash table migration enjoys the help of more than one thread, but requires a step in which the leader performs the necessary alterations to the data table before other threads can join in the migration.

The addition of this mechanism ensures that the dictionary presented here is capable both of performant lock-free operations, and of complex operations that require the exclusive access to the entire dictionary in order to be performed without a prohibitive number of expensive atomic memory writes.
The properties that a synchronous operation is sequential and ensured to be mutually exclusive w.r.t.\ all other threads, make it also very simple to be explained and understood, an important characteristic for collaborative development.

Do note that the presence of a synchronous operation does not obstruct concurrent lookup operations, as also described in the relevant later sections.
The meaning is that for concurrent migrations, the lookup linearizes on the state of the hash table prior to the migration (the thread performing the lookup will not be participating in the migration process).
For other arbitrary synchronous operations, in principle this behavior may not be acceptable; nevertheless, for those considered so far, it is indeed acceptable.
This exemption of lookups from the participation in migrations follows along the lines of~\cite[\S5.3.2, Preventing Concurrent Updates to Ensure Consistency]{maier}, i.e.\ only mutations are prevented, read-only operations are allowed to proceed on the prior state of the hash table.


\subsection{Lazy Robin-Hood}\label{subsec:lazy-robin-hood}

The Robin Hood state for the hash table is essentially the distance each key has from its distance-0 position.
In other words, the number of collisions.

In order to keep track of this distance, we steal some bits from the nodes in the index.
According to~\cite[Corollary to Theorem~3]{robin-hood}, the expected number of collisions per probe for a full table is $\Theta(\log n)$, e.g.\ four for a table of size 64.
Such is a relatively low number, thus we can steal two bits, or in general $\log \log n$ bits, from the index nodes.

We do so, by following the values in Table~\ref{tab:robin-hood-nodes}.
As you can observe the proposed hash table has definite minimum and maximum sizes, respectively of $2^{6} = 64$ and $2^{56} \approxeq 7.2 \times 10^{16}$.

\begin{table}
	\centering\begin{tabular}{llll|llll}
		$\log n$ & Node & Distance & Tag & $\log n$ & Node & Distance & Tag\\
		\hline
		6 & 8 & 2 & 0 & 32 & 64 & 5 & 27 \\
		7 & 16 & 3 & 6 & 33 & 64 & 6 & 25 \\
		8 & 16 & 3 & 5 & 34 & 64 & 6 & 24 \\
		9 & 16 & 4 & 3 & 35 & 64 & 6 & 23 \\
		10 & 16 & 4 & 2 & 36 & 64 & 6 & 22 \\
		11 & 32 & 4 & 17 & 37 & 64 & 6 & 21 \\
		12 & 32 & 4 & 16 & 38 & 64 & 6 & 20 \\
		13 & 32 & 4 & 15 & 39 & 64 & 6 & 19 \\
		14 & 32 & 4 & 14 & 40 & 64 & 6 & 18 \\
		15 & 32 & 4 & 13 & 41 & 64 & 6 & 17 \\
		16 & 32 & 4 & 12 & 42 & 64 & 6 & 16 \\
		17 & 32 & 5 & 10 & 43 & 64 & 6 & 15 \\
		18 & 32 & 5 & 9 & 44 & 64 & 6 & 14 \\
		19 & 32 & 5 & 8 & 45 & 64 & 6 & 13 \\
		20 & 32 & 5 & 7 & 46 & 64 & 6 & 12 \\
		21 & 32 & 5 & 6 & 47 & 64 & 6 & 11 \\
		22 & 32 & 5 & 5 & 48 & 64 & 6 & 10 \\
		23 & 32 & 5 & 4 & 49 & 64 & 6 & 9 \\
		24 & 32 & 5 & 3 & 50 & 64 & 6 & 8 \\
		25 & 32 & 5 & 2 & 51 & 64 & 6 & 7 \\
		26 & 64 & 5 & 33 & 52 & 64 & 6 & 6 \\
		27 & 64 & 5 & 32 & 53 & 64 & 6 & 5 \\
		28 & 64 & 5 & 31 & 54 & 64 & 6 & 4 \\
		29 & 64 & 5 & 30 & 55 & 64 & 6 & 3 \\
		30 & 64 & 5 & 29 & 56 & 64 & 6 & 2 \\
		31 & 64 & 5 & 28 & \\
	\end{tabular}
	\caption{Sizes of internal fields of nodes in the index hash table. Let $n$ be the capacity of the table, and \emph{Node}, \emph{Distance}, and \emph{Tag} refer to their respective sizes, expressed in number of bits.}
	\label{tab:robin-hood-nodes}
\end{table}

Comparing with~\cite{robin-hood}, we have slightly modified the invariant so as to favor avoiding moving elements on a tie.

The laziness in our approach is that if the \emph{aligned} size of the write required to safely insert the node into the index exceeds 128-bits, then we refrain from maintaining the Robin Hood invariants at all, for we would need to employ more than one atomic writes in order to store the newly inserted key.
This is because we cannot use the hardware~\cite[CMPXCHG--Compare and Exchange]{x86-64} instruction over more than 16-bytes.
The instruction further requires that the write be 16-byte aligned, or else a segmentation fault is encountered.
(Other faults may be encountered as well, depending on the architecture; for instance in \texttt{aarch64} ISAs a bus fault is encountered.
ISAs other than x86-64 are not currently supported; see~\S\ref{subsec:compatibility-with-other-isas}.)
Thus, e.g.\ if a new node is inserted into the index after a 16-byte boundary, the insertion cannot be performed with a normal (i.e.\ compact) node.

That is, when the distance cannot be stored in $O(\log \log n)$ space, we semantically say the distance is $\infty$ and store it as $\log \log n - 1$ (we may later refer to this as the maximum distance for a given size of the table, or simply as the maximum distance).
When a node has distance equals to the maximum distance, we say that node is a non-compact node.
We also employ non-compact nodes to denote tombstones.
As is well known, see for instance~\cite[\S6.4]{the-art-vol-2}, ``the obvious way of deleting records from a hash table doesn't work.''
Instead of simply removing a node, we swap it for a tombstone node, which is a non-compact node pointing to entry number 0 of the data table.
The data table maintains the invariant that the key and value fields of that entry are always \texttt{NULL}.
This is achieved by requiring at initialization time that the thread creating the \texttt{AtomicDict} instance reserves entry number 0 for itself, and marks it as already occupied, without writing anything into it.
Thus, all data tables have at least one unused entry.
Furthermore, if the thread that created the hash table never writes into it, there will always be $|R|$ unused entries, with $|R|$ being the size of the reservation buffer.
For more details on reservations see \S\ref{subsec:reservation-buffers}.
Operations that read the index, simply always skip non-compact nodes that point to entry 0.
Further details on tombstone nodes and deletes are given in \S\ref{sec:delete}.

When a new node could be inserted, respecting the requirements for our lazy robin hood, but it should be inserted at a distance greater then the maximum, we initiate a grow migration, as described in~\S\ref{sec:migrations}.
In effect, we have a local trigger for our migrations that doesn't require global knowledge of the table.
We can observe the local effect of collisions to determine that the table size is too small, and take action accordingly.
This is not the only trigger for migrations, for a complete list refer to the relevant section.
One may argue that a degenerate set of hashes can continuously trigger migrations.
While this is true in principle, do observe an implication of Table~\ref{tab:robin-hood-nodes}: starting from a table capacity of $2^9 = 512$, the maximum distance is 16, while the number of nodes in a 16-byte-aligned region of memory (a quad word), is 8.
Therefore, the described mechanism effectively cannot be triggered indefinitely.


\subsection{Garbage Collection}\label{subsec:garbage-collection}

As Micheal Maged put it in~\cite[\S2.3]{micheal-hash-tables}:
\begin{quote}
	[\ldots] The failure or delay of the garbage collector may prevent threads operating on lock-free objects from making progress indefinitely, thus violating lock-freedom.
\end{quote}

Such is undeniably correct, and therefore, the proposed implementation for \texttt{AtomicDict} cannot be considered lock-free, simply by virtue of the fact that its memory is managed through Python's garbage collector.
Notwithstanding, this was a deliberate implementation detail and not a foundational piece in the design of \texttt{AtomicDict}.
The fact that its internal data structures are traced with Python's GC is something that can be easily changed without compromising the design.
The choice was chiefly made to simplify the implementation of \texttt{AtomicDict}.
Furthermore, given the fact that the hash table access was subject to the interpretation of Python code by the CPython interpreter, as that is the target for the presented work, there was no possibility of not having to deal with a stop-the-world GC\@.

Python code is not the only means through which \texttt{AtomicDict}'s APIs can be accessed.
The library also exposes C header files (though, their standalone usage has not yet been explored).
For those use-cases that don't employ Python code, but rather directly call \texttt{AtomicDict}'s C functions, true lock-freedom can be of interest.

Changing the implementation to not be based on the provided GC, but instead using a dedicated memory management system, is entirely possible; albeit requiring a lot of additional work.
One way this can be achieved is by implementing what Micheal himself describes in~\cite{micheal-safe-reclamation}, which employs so-called \emph{hazard pointers} (or any other suitable algorithm).

Serious effort was not put into looking at this possible implementation change, as of the time of writing.


\section{Lookup}\label{sec:lookup}

The proposed lookup design is what could probably be expected given the discussed design of the hash table.
It departs sharply from Maier's, as it should be expected, and, in fact, it is not exactly the same as Python's either, due to the employment of Robin Hood hashing, and lack of key-sharing.
A fairly complete C-like pseudocode of its implementation is reported in Listing~\ref{lst:lookup}.
The operation begins by computing the $d_0$ index slot, based on the pre-computed hash, by calling the \texttt{Distance0Of} function, which internally re-hashes the Python-generated hash, as described in~\ref{subsec:maier}, and then computes the slot position, based on the size of the hash table, listed in the \texttt{meta} object.
The hash computation is deliberately omitted from Listing~\ref{lst:lookup}, because it simply entails making the correct call to the relevant Python C-API, and checking for possible errors.

The algorithm then looks up the index for the searched element, starting from position $d_0$, until either:
\begin{enumerate}
	\item the item is found;
	\item an empty slot is found (the only probe in which the item could be found was looked at in its entirety);
	\item the hash table is in a compact state (i.e.\ no non-compact nodes are in the index) and the search can be pruned due to the Robin Hood invariant (i.e.\ the distance of the read node is greater than what the distance of the searched node would be if it was to be found); or
	\item the probe distance is equal to the index size.
\end{enumerate}

The searched key can be found if
\begin{enumerate}
	\item there is a node in the index whose tag corresponds to the key's hash;
	\item the node points to an entry other than 0;
	\item the entry's value is not \texttt{NULL};
	\item the stored (Python-generated) hash is not different from the key's hash; and either
	\begin{enumerate}
		\item the pointers of the searched key and the stored key are the same (identity); or
		\item the keys are semantically equal, based on the arbitrary logic with which those two Python objects may be deemed equal.
	\end{enumerate}
\end{enumerate}

Comparisons in Python are notoriously costly.
As is well known, Timsort was implemented specifically for Python, with the explicit goal of minimizing the amount of comparisons as much as possible.
It can be seen that in lines 38--55, care is taken in order to delay the call to the comparison function as much as possible.
This is because comparisons between arbitrary Python objects can be very computationally expensive.
Since the involved object's types may be implemented in Python code, it can require interpreting an arbitrary Python function.
Such function does not have any restrictions on its computational complexity, and even if it was enforced to be an $O(1)$ operation (as it would be normally expected), its constant factor would still be very high, due to interpretation overhead.
The comparison itself may also raise an exception, either if the two objects' types are not comparable, or if the arbitrary Python code that was executed to compare the two objects raised an exception for any reason (a circumstance handled in line 57).

Do note that inside the \texttt{ReadEntry} sub-routine the key is always read before the value.
This avoids the problem of torn-reads, exactly as described in~\cite[\S4, Lookup]{maier}.

On line 69 the eventuality that the compact state of the hash table has changed is taken into account.
Do note that for a generation of the hash table the only transition that can occur to the compact state is from $\top$ to $\bot$.
In order to restore the compact state (from $\bot$ to $\top$), a migration is required.
(The concept of \emph{generation} is introduced in the Migrations Section~\ref{sec:migrations}.)
Suppose the check on line 69 didn't exist, and consider two threads $t_l$ and $t_i$ executing respectively a lookup for key $k$ and an insert for the same key.
There can exist an execution s.t.:
\begin{enumerate}
	\item $t_l$ runs first and reaches line 8, reading that \texttt{is\_compact} $= \top$;
	\item $t_i$ runs from start to end, turning \texttt{{meta->is\_compact}} $= \bot$, and inserting $k$ with a non-compact node; and
	\item $t_l$ runs again, not finding the node pertaining to $k$, thus returning a lookup failure.
\end{enumerate}
Instead of this evidently faulty behavior, the check on line 69 makes the lookup restart from the beginning, so that item (3) above would be substituted with
\begin{enumerate}
	\item[3.] $t_l$ at first not finding $k$, sees that a non-compact insertion could have linearized, undermining its assumption that the Robin Hood invariant holds, and thus restarts from line 7, eventually finding $k$.
\end{enumerate}

Another similar check can be found on lines 72 and 79, where concurrent migrations are considered.
It is important to note that lookups are permitted to run concurrently with migrations, as they are read-only operations.
When a concurrent migration from the current generation of \texttt{meta} to a newer generation has been linearized (setting this metadata's \texttt{{migration\_done}} flag to $\top$) it is possible that a subsequent insert for the same key as the lookup operation linearizes before the lookup completes, in a manner similar to the previously discussed compactness of the hash table.
In such scenarios, the lookup operation is restarted.
In this case, the $d_0$ position of the key must also be recomputed.

Towards the end of Listing~\ref{lst:lookup}, at lines 68--94, is where the response is written into an externally allocated C \texttt{struct}.
Given the fact that this routine is also employed elsewhere (see Listing~\ref{lst:delete}, line 8), it would be insufficient to return a simple boolean value.
Instead the following is returned, which completely describes the state of the relevant memory pertaining to the search key:
\begin{enumerate}
	\item an \texttt{error} flag, if $\top$, the rest of the values returned are undefined;
	\item a \texttt{found} flag, if $\bot$, the returned values below are undefined;
	\item an integer \texttt{position}, indicating at which slot of the index the relevant node was found;
	\item an \texttt{{is\_compact}} flag, indicating whether the node was found to be compact in the index;
	\item a \texttt{node} structure, with the node that was found;
	\item an \texttt{{entry\_p}} pointer, to the relevant data table entry; and
	\item an \texttt{entry} structure, with the values read from the data table entry.
\end{enumerate}

\lstinputlisting[
	numbers=left,
	stepnumber=5,
	numberfirstline=true,
	language={C},
	caption={Lookup Operation},
	label={lst:lookup}
]{listings/lookup.c}

\subsection{Linearization}\label{subsec:lookup-linearization}

A successful lookup linearizes on line 41, where the key and value are read.
Failed lookups, i.e.\ lookups that don't find the searched key, linearize on line 15, where a node is read from the index, be it an empty node, or a node $n$ whose distance-0 position $d_0(n) >$ distance-0 position of the searched key.


\paragraph{Lock-freedom.}

It is trivial to see that no thread's arbitrary delay can induce this lookup to also be subjected to an arbitrary delay, insofar as the code presented is concerned.
Thus, the lookup operation is lock-free.

It is true that there is one aspect that evades the control of the author, namely the equality comparison of line 55.
It can indeed be that the arbitrary code that is run also comprises a lock acquisition.
Lock-freedom of that relevant code can in no way be evinced, nor enforced.
Therefore, we can only assume (or rather, hope) that the arbitrary code is sane and does not exhibit such surprising behavior.

The \texttt{Lookup} above described is not wait-free.
In fact, the check at line 69 makes it so that when a concurrent insert sets the metadata's \texttt{{is\_compact}} flag to $\bot$, and the similar check for migrations on line 72, the lookup operation needs to be retried.


\section{Insert or Update}\label{sec:insert-or-update}

The \texttt{ExpectedInsertOrUpdate} routine shown in Listing~\ref{lst:insert} departs from Maier's design of~\cite[Algorithm~1]{maier} in that it doesn't expect the user to provide an update function, but rather mimics more the compare and set routines of other atomic data structures: it takes as input the expected and desired values, and it returns the value that was stored before the update was performed (or a special \texttt{{NOT\_FOUND}} object).
If the expected value was not the stored value (or the compare and set low-level call failed and the newly read value then differed from the expected one), then a special \texttt{{EXPECTATION\_FAILED}} object is returned.
A routine that more closely resembles Maier's Algorithm~1, is described later in~\S\ref{sec:aggregate}, based on the one discussed here.

There's also another special value called \texttt{ANY} that can be used as the expected value to signify that whatever the current value is, even an absent value, it should be replaced with the desired value.
Returning the previous value, in conjunction with \texttt{ANY} as the expected value, also serves to reuse the same routine to behave like a swap, rather than a CAS\@.

The three special objects \texttt{{ANY}}, \texttt{{NOT\_FOUND}}, and \texttt{{EXPECTATION\_FAILED}} cannot be used as keys or values in \texttt{AtomicDict}, so as to guarantee their semantics are consistent (cf.\texttt{\ {EXPECTATION\_FAILED}} being a value returned by this routine).
This is enforced, but not otherwise shown here.

If the expected value is either \texttt{{ANY}} or \texttt{{NOT\_FOUND}}, there is an insertion fast-path used in which the $d_0$ element in the index is CASed with the entry reserved without going through the currently stored data in the index.
If this CAS succeeds, then the operation is completed.
If it doesn't the general code path follows.
Notice that this doesn't compromise correctness: if the $d_0$ node was in fact empty, it means that the key wasn't in the hash table.

Here, it is worth mentioning that this routine is not directly exposed, and is instead considered internal.
Instead of calling this function directly, a user of \texttt{AtomicDict} is expected to call different Python idioms, which all internally call \texttt{ExpectedInsertOrUpdate}; namely:

\begin{enumerate}
	\item \texttt{{d[k] = v}}, eq. \texttt{{ExpectedInsertOrUpdate(\ldots, k, \ldots, ANY, v, \ldots)}};
	\item \texttt{{d.compare\_and\_set(k, exp, des)}}, eq. \texttt{{ExpectedInsertOrUpdate(\ldots, k, \ldots, exp, des, \ldots)}}, which raises an exception when the expected value was not the stored value;
	\item \texttt{{d.swap(k, v)}}, eq. \texttt{{ExpectedInsertOrUpdate(\ldots, k, \ldots, ANY, v, \ldots)}}, which is similar to (1) except it also exposes the previous value.
	(Not yet implemented.)
\end{enumerate}

In regards to (2) above, returning a boolean value is generally considered the standard.
Such is what the hardware primitive essentially does, and also what other languages such as Java do.
Nevertheless, in order to favor the usage of the Aggregate routine of~\S\ref{sec:aggregate}, which correctly implements a generic mutation to a value, it was chosen to expose this behavior instead.
That is, simpler usages, such as incrementing a counter, or any other ``aggregations,'' should instead employ the more appropriate \texttt{aggregate} method.
This \texttt{compare\_and\_set} method is thus intended to be used in scenarios where the expectation is ``taken for granted,'' and would otherwise require debugging if unmet; that is, where the failure of the expectation has more severe impacts than mere contentious access.
Do note that this design does not pertain to any correctness or performance goal; it, instead, has to do with the programmer's ergonomics in the usage of this lower-level routine.
Not every programmer possesses thorough knowledge of concurrency issues, and an interface that alleviates some of those complexities is what the author strives to achieve.
Whether or not this is actually simpler to use remains to be seen.

The \texttt{ExpectedInsertOrUpdateCloseToDistance0} sub-routine, called at line 35, is omitted for brevity.
What it does can be described with the following steps:
\begin{enumerate}
	\item consider the nodes in the 16-byte-aligned region of memory the $d_0$ slot is part of, in sequence, starting from $d_0$:
	\begin{enumerate}
		\item if an empty slot is found either set \texttt{expectation} $=\bot$, if \texttt{expected} is neither \texttt{ANY} nor \texttt{{NOT\_FOUND}}, or attempt to insert the node in keeping with the Robin Hood invariant;
		\item if a non-empty slot is found, check the tag, and possibly call the \texttt{ExpectedUpdateEntry} routine;
	\end{enumerate}
	\item set the \texttt{{must\_grow}} flag if the node would be inserted at a distance greater than the current maximum distance (and avoid modifying the index);
	\item set the \texttt{done} flag accordingly.
\end{enumerate}

The \texttt{ExpectedUpdateEntry} sub-routine is also omitted for brevity; its implementation resembles the \texttt{{check\_entry}} part of the \texttt{Lookup} routine (Listing~\ref{lst:lookup}, lines 38--65), but also performing an update, possibly signaling that the expectation failed.


\lstinputlisting[
	numbers=left,
	stepnumber=5,
	numberfirstline=true,
	language={C},
	caption={ExpectedInsertOrUpdate Operation},
	label={lst:insert}
]{listings/insert.c}

\subsection{Linearization}\label{subsec:insert-linearization}

The presented \texttt{ExpectedInsertOrUpdate} operation is linearizable.

A successful insertion linearizes upon the insertion of the new node into the index (note that a data table entry for an insertion is prepared beforehand, and possibly cleaned up later if deemed unnecessary, that is, if the routine turned into an update).
Before that happens, concurrent lookups and deletes have no visibility on the data table entry which hosts the yet-to-be inserted key, since their search is based on a hash lookup on the index before looking into the data table.
After it happens, either with a successful CAS on line 56 (non-compact) or equivalently with a successful CAS inside the compact sub-routine, concurrent lookups will see the new node, and visit the pointed-to data table entry, successfully finding the key.
(Consider a call to \texttt{ExpectedInsertOrUpdateCloseToDistance0} to essentially boil down to a laborious computation, before picking the right value with which to call CAS\@.
Thus, externally it can be observed to behave in much the same way as a non-compact insertion.)

Pertaining concurrent iterations (described later in Sections~\ref{sec:iterations} and~\ref{sec:consistent-iteration}), which mainly focus on reading the data table, the linearization point thus described still holds.
When an iteration visits an entry, it additionally checks for the relevant node in the index to be present and if it is not, then it avoids yielding that value.

\subsection{Lock-freedom}\label{subsec:insert-lock-freedom}

The described \texttt{ExpectedInsertOrUpdate} is lock-free, inasmuch as it does not trigger a grow migration.

As mentioned earlier, we maintain the lock-freedom property only for those operations that don't necessitate migrations to complete.
As described, this is because the implementation of completely lock-free hash tables is deemed impractical.
Where migrations are not concerned, this routine is indeed lock-free: no thread's arbitrary delay can arbitrarily delay this routine as well.
Later, we will not specify this caveat of lock-freedom holding only when no migrations are in progress; such is also sustained by Maier et al.\ in~\cite{maier}.

\texttt{ExpectedInsertOrUpdate} is not wait-free.
An insert may in fact be delayed in several points:
\begin{enumerate}
	\item when making a reservation in the data table (which is not shown, for brevity, in Listing~\ref{lst:insert});
	\item when failing to make use of the insert quick-path of line 25;
	\item when the CAS inside the \texttt{ExpectedInsertOrUpdateCloseToDistance0} sub-routine fails, due to the index nodes involved being contended;
	\item when a non-compact node fails to be inserted on line 56, again due to contention on the index position;
	\item when a concurrent update or deletion changes the already-present key (thus, this would be an update, rather than an insert); and
	\item when the check on line 103--104 fails, due to another thread changing the index state to non-compact.
\end{enumerate}
Note that the delays here mentioned do not cause obstruction for a running \texttt{ExpectedInsertOrUpdate}, which remains lock-free, notwithstanding the contention issues listed above.


\section{Delete}\label{sec:delete}

Albeit an infrequently used feature of hash tables, according to the data shown in~\S\ref{sec:dict-metrics}, supporting deletions has taken a substantial portion of the development time for the implementation of \texttt{AtomicDict}.
Several design iterations have been evaluated, and are not explored here in detail.
The current proposal, which is not fully implemented as of the time of writing, is what follows.

Deleting a key $k$ from the hash table is considered linearized when the value, i.e.\ a pointer to a \texttt{PyObject} stored in the data table entry, associated with $k$ is set to \texttt{NULL}.
Being such a conceptually simple semantics for deletes, this is the part that has remained constant throughout the iterations on the design.

After the delete is linearized, the following housekeeping actions are taken:
\begin{enumerate}
	\item the relevant index node is exchanged for a tombstone, so as to avoid requiring lookups from visiting the data table entry;
	\item a synchronous operation may be started so as to defragment the data table; and
	\item in addition to the above synchronous operation, a shrink migration may be triggered as well, if the estimated size, given by the upper-bound formula in~\S\ref{sec:approximate-size}, is less than the minimum fill ratio (i.e.\ 1/3 by default.).
\end{enumerate}

The three additional steps are carried out in the delete routine, because:
\begin{itemize}
	\item deletes are considered infrequent, so adding cost to them and relieving other routines of this work is considered to be a decrease in cost overall (as is the case for the tombstone exchange); and
	\item if deletes happened to be frequent instead, it would likely be that the program expects the memory consumption of the hash table to decrease (which is what we do by defragmenting the data table and triggering a shrink migration).
\end{itemize}


\subsection{Exchanging the node for a tombstone}\label{subsec:exchanging-node-tombstone}

The swapping for a tombstone node is fairly simple and is shown in lines 33--36 of Listing~\ref{lst:delete}.
Recall that doing this swap instead of simply deleting the node is necessary, as already described in~\S\ref{subsec:lazy-robin-hood}, and in~\cite[\S6.4]{the-art-vol-2}.
In our circumstances, it is especially useful in order to help concurrent lookups: the thread looking up a key $k$ may very well visit the data table entries pertaining to the visited nodes in the index, possibly finding that $k$ had been deleted by reading that its associated value $=$ \texttt{NULL}.
But reading a data table entry is much more costly than reading a node in the index, given that the index nodes are much smaller in size compared to a data table entry and that the next node index to visit is very likely to already be in the processor's cache, while the entry is likely not.

The \texttt{LookupEntry} routine, omitted for brevity, is essentially equivalent to the \texttt{Lookup} routine shown in Listing~\ref{lst:lookup}, but instead of looking for a node that points to an entry that contains the searched key, it looks for a node that points to a certain fixed location of the data table; thus it is an operation that only involves reading the index.

The tombstone node is possibly moved further down its probe, if such is possible within a single atomic write; that is, as long as the atomic write is of at most 16-bytes, and is 16-bytes aligned.


\subsection{Defragmenting the data table}\label{subsec:defragmenting}

Keep a per-page deletions counter noted as $\delta(p)$, for some page $p$.
After a delete, atomically increment $\delta(p)$.
If $\delta(p) \geq \frac{|P|}{2}$, initiate a merge.

The following description is what occurs after a merge operation has been triggered.
\begin{enumerate}
	\item Look for another page $p'$, distinct from the page from which the merge was triggered, s.t. $\delta(p') \geq \frac{|P|}{2}$.
	\begin{enumerate}
		\item Since page number 0 cannot be cleared because it hosts entry number 0, which has to remain for the correctness of tombstones, start the search from page number 1 (these checks are omitted for brevity from Listing~\ref{lst:delete}); and
		\item if no such page can be found, abort the merge operation.
	\end{enumerate}
	\item W.l.o.g.\ call $p_1$ the lower-numbered page, and $p_2$ the other.
	\item Let $p_0$ be the lowest non-empty page in the data table.
	It may be that $p_0 \equiv p_1$, and that is fine.
	\item Move the elements found in $p_0$ into $p_2$.
	When $p_2$ becomes full, move the remaining elements into $p_1$ instead.
	(Note that if $p_0 \equiv p_1$, then $|p_0| = |p_1| \leq \frac{|P|}{2}$, and also $|p_2| \leq \frac{|P|}{2}$, therefore $|p_0| + |p_2| \leq |P|$.
	Thus, all elements from $p_0 \equiv p_1$ will be moved into $p_2$.)
	That is, $\forall e \in p_0$:
	\begin{enumerate}
		\item clear $e \in p_0$;
		\item write $e$ into a free slot of $p_2$ (possibly overwriting deleted slots);
		\item if $e$ is a reservation with unoccupied slots left:
		\begin{enumerate}
			\item write the unoccupied slots too;
			\item look into every accessor's storage (this is already a Synchronous Operation) and edit the relevant reservation to point to the new slot; and
		\end{enumerate}
		\item update the relevant node in the index to point to the new location.
	\end{enumerate}
	\item Update $\delta(p_0)$, $\delta(p_1)$, and $\delta(p_2)$ accordingly.
\end{enumerate}

The relevant pseudocode can be found in Listing~\ref{lst:delete}, lines 40--87.
Note that step (4) above is summarized on line 70.


\subsection{Triggering a shrink migration}\label{subsec:triggering-a-shrink}

After a merge operation completes, as described in the above Section, a shrink migration may be triggered, following the increment of the greatest deleted page field in the metadata.
It would cause the size of the index and of the pages array to be reduced, if usage is below a certain threshold, which by default is of 1/3, consistent with Python's \texttt{dict} defaults.

The thread caller of \texttt{Delete} is responsible for actually initiating the shrink migration, after having yielded its accessor's lock.


\subsection{The current implementation}\label{subsec:the-current-implementation}

The current implementation, which is not shown here, is essentially similar to the one described so far, but instead of the page-merge algorithm, defragmentation is carried out by swapping entries.
Which is the implementation that can be found in~\cite{cereggii}, available online at \url{https://github.com/dpdani/cereggii/blob/40eb43084232263716dc5e7fe753253fdc042e6b/src/cereggii/atomic_dict/delete.c}.\footnote{%
	Last accessed \today.
}
When a key is deleted, its entry is swapped with another entry stored at a lower-numbered location, so as to maintain the possibility of eventually incrementing the deleted pages counter ($P_D$ in~\S\ref{sec:approximate-size}).
Notice how the necessity for defragmentation remains, in order to provide a fairly cheap and wait-free approximation of the size, which is required by the \texttt{Delete} and \texttt{ExpectedInsertOrUpdate} routines in order to know when to trigger a migration.
Other mechanisms for triggering migrations that don't require defragmentation could have been adopted, but this one is particularly useful because it also makes iterations faster, by reducing the necessity of an iterating thread to visit empty entries of a fragmented data table.

A way for the swapping operation to be carried out safely in spite of concurrent mutations was found.
At first the deleted entry, already having value $=$ \texttt{NULL}, would be marked as entombed, so that successive unfinished modifications due to the swap mechanism would not be read as valid entries by other threads.
Then, a swapping entry would be looked for in the data table, at a lower-numbered location.
The swapping entry would be flagged as such, so that no other concurrent delete may try to swap the same entry.
Then, the key and value of the swapping entry would be copied to the deleted entry, whose entombed state would be kept.
The value of the swapping entry would be substituted with a flagged pointer to the entombed entry, which would signal to a thread that is reading the swapped entry that it should go and read the other entry.
Finally, the index node pointing to the swapped entry is modified to point to the previously entombed entry.

The above is considered possible to implement with the lock-freedom property.
Nevertheless, it exhibits a problem concerning torn reads: suppose a thread reads the key part of the entry before the entry is marked as deleted, then it gets deleted and swapped, the thread that read the previous key now proceeds and reads the swapped value, thinking that it pertains to the prior key.
However much unlikely this scenario may appear to be, it still needs to be considered, given that the hardware does not generally expose double-word atomic reads.

Swapping entries to defragment unoccupied slots of data table pages, requires a lot of care.
This renders the implementation of deletes much harder to explain and to maintain over time, possibly by people other than the present author.
It also doesn't provide any immediate benefit: memory reclaim is deferred to the next shrink anyway, whereas merges being Synchronous Operations (albeit shorter operations compared to shrinks), means that memory is reclaimed sooner and more frequently.

\lstinputlisting[
	numbers=left,
	stepnumber=5,
	numberfirstline=true,
	language={C},
	caption={Delete Operation},
	label={lst:delete}
]{listings/delete.c}

\subsection{Linearization}\label{subsec:delete-linearization}

The presented \texttt{Delete} operation is linearizable.
Its semantics (i.e.\ removing a key $k$ from the key-set $K$), are observable instantaneously at line 18 when the value associated with $k$ is set to \texttt{NULL}, with an atomic CAS operation.
Such is for a successful invocation, i.e.\ one that actually removes $k$ from $K$.
Whereas a failed invocation, i.e.\ one that finds $k \not\in K$, is linearized inside the call to the \texttt{Lookup} routine of line 8.
With the \texttt{Lookup} routine being already shown linearizable in~\S\ref{subsec:lookup-linearization}, it follows that also a failed invocation of a delete operation is linearizable.

\texttt{Delete} is linearizable also in spite of concurrent deletions of the same key $k$.
Under such circumstances, the successful CAS of line 18 by the one successful invocation linearizes that call to \texttt{Delete}, as above; while the failed CAS of but the same line 18, will result into the visiting of line 22, forcing the delete operation to be retried, and then fail, as aforementioned.


\subsection{Lock-freedom}\label{subsec:delete-lock-freedom}

The presented \texttt{Delete} operation is lock-free, when defragmentations or migrations are not required.
(And when they are, the linearization point happens before the non-lock-free part of deletions begins.)
\texttt{Delete} is not wait-free, because a delete for a certain key $k$ by thread $t_1$ may be delayed by a concurrent delete, for the same key $k$ by another thread $t_2$.
(It may also be delayed by a concurrent update, but the presented reasoning is sufficient to prove the lack of wait-freedom.)
When that happens, $t_1$ is delayed in performing a retry, as specified in the pseudocode at line 22.
If the same key $k$ is continuously deleted and re-inserted by threads other than $t_1$ it may be that $t_1$ is delayed indefinitely, since there is no mechanism for fairness.
It follows that the presented deletion is not wait-free.


\section{Migrations}\label{sec:migrations}

When the hash table becomes too full ($\approx 2/3$ used slots), or too empty ($\approx 1/3$ used slots), a migration is triggered by the insert and delete routines, respectively.
At its core, such is necessary because it is not possible to simply add more slots into an array after it has been allocated; and the index and data table pages array are in fact regular arrays.
Thus, in order to allow for more keys to be inserted, or for memory to be reclaimed, it is necessary to substitute the aforementioned arrays.

\texttt{AtomicDict} allows for three kinds of migrations: \emph{grow}, \emph{shrink} and \emph{compact}.
Respectively, they allow:
\begin{enumerate}
	\item to double the maximum possible size of the key-set,
	\item to halve it, and
	\item in contrast with the other two, to recover the Robin Hood invariants, which may have been lost due to our Lazy Robin Hood approach, described in~\S\ref{subsec:lazy-robin-hood}.
\end{enumerate}
The three kinds are slightly different from each other in their implementation, but are externally signalled in the same manner: by setting a field in the current metadata to show that there is a migration in progress.

Migrations are executed entirely or in part within a Synchronous Operation.
The application of a Synchronous Operation relieves the implementation from being concerned with the following problem, and its variations.
\begin{enumerate}
	\item A thread $t_i$ is performing an insertion of a key $k$.
	\item Another thread $t_m$ is performing a migration to a newly allocated index.
	\item $t_i$ reaches its linearization point before the migration completes.
	\item Has $k$ been migrated?
\end{enumerate}
Instead, it becomes much easier to implement, and maintain, a logic in which thread $t_m$ initiates a Synchronous Operation, so that all other threads either complete their mutation before the migration starts, or know that a migration is in progress and help carry it out before resuming their work.
This is effectively the same approach expressed in~\cite[\S5.3.2, Preventing Concurrent Updates to Ensure Consistency]{maier}.

One may argue that, in principle, updates (as opposed to inserts) don't need to wait for a migration to complete, since all they need to do is to update the value pointer in the relevant data table entry.
Nevertheless, the update might need to be retried in case it returns that the searched key was not found, and if a concurrent migration completed in the meantime.
Additionally, a migration might in fact leave the data table untouched only if it happened to be a grow migration.
A compact, or shrink migration will instead try to reduce the size of the pages array of the data table, thus actually performing some operations on both the data table and the index.
Depending on the migration's current state, it may be that the update is referred by an index node to a wrong entry in the data table.
Therefore, updates are obstructed by migrations, in the same manner as inserts.


\subsection{Metadata}\label{subsec:metadata}
Most of the \texttt{AtomicDict} data is stored inside its metadata, which holds references to the index, and to the data table, among other useful things.
An external observer may perceive a migration as a change from a prior metadata object to a new one.
We will also refer to them as the current generation metadata and its successor, or the new generation metadata.

In addition to the aforementioned pointers, which are the core parts of metadata objects, are also stored therein:
\begin{enumerate}
	\item the size of the hash table (a power of 2);
	\item the pages counters (see~\S\ref{sec:approximate-size});
	\item the \texttt{is\_compact} flag;
	\item various information about the sizes of index nodes and their internal parts (see Table~\ref{tab:robin-hood-nodes});
	\item a pointer to the next metadata generation;
	\item the migration leader thread id (also see~\S\ref{sec:free-threading});
	\item the next node to migrate counter (or, in Maier's terminology, the blocks counter, that is, the next node to migrate is effectively the first node in what Maier denotes as a block);
	\item three atomic events used to signal the progress of the three phases of migrations (one metadata's events are only used for the migration which sees it as the current generation, since they are single-use by design, also see~\S\ref{sec:free-threading}); and
	\item other miscellaneous information.
\end{enumerate}

The metadata object is stored inside an \texttt{AtomicRef}, described in detail in Appendix~\ref{ch:atomic-reference}.
It essentially provides an interface for an atomically updatable reference to a \texttt{PyObject}, that also handles correct \texttt{PyObject} concurrent reference counting.
Specifically, the objects stored therein are subject to QSBR, and thus are not immediately freed upon reaching a reference count of 0.
See~\S\ref{sec:qsbr} for a discussion on QSBR\@.
This behavior is important for metadata objects, because it may be that a thread $t$ reads the pointer to the current metadata, and a concurrent migration replaces it with a newer generation, before $t$ manages to increment the metadata's reference count.
Thus, not having QSBR for metadata objects would make the implementation of \texttt{AtomicDict} subject to use-after-free bugs.

\subsection{Migration Leader}\label{subsec:migration-leader}
\subsection{Migration Phases}\label{subsec:migration-phases}


Mutation operations are blocked while a migration is in progress.
This is done by starting a Synchronous Operation.
The first phase of a migration is to prepare the new generation metadata.
\begin{itemize}
	\item In a shrink or compact migration, the metadata is ready after the leader thread allocates it, creates the new index, clears it (i.e.\ \texttt{memset(\ldots, 0, \ldots)}), allocates the new, smaller pages array, and copies the non-deleted pages into the new pages array.
	\item In a grow migration, the metadata is ready after the leader thread allocates it, creates the new index, and copies the pages array into a new, larger pages array.
\end{itemize}
After the new generation metadata is ready, the follower threads may start migrating the nodes from the current generation to the new one.

The Synchronous Operation is here ended by the leader thread for shrink and compact migrations, in order to allow new accessors to join the migration process.
Whereas for grow migrations, this is not done, and instead the Synchronous Operation is ended only after the current generation metadata is swapped with the new one.
The reason for this limitation is that the threads that participate in a grow migrations essentially need to hit a barrier and wait for all participants to have hit it.
In order for this to work, we need to know in advance the amount of threads that will need to hit the barrier.
Thus, we cannot allow the possibility of new accessors joining the migration process after it has started.
This is not necessary for shrink and compact migrations because they use a different semantic in order to determine whether the migration has completed.
Namely, that the generation tag of each page in the data table is set to the new generation.


\subsection{Slow Migrations}\label{subsec:slow-migrations}

As it can be easily evinced from the rest of this Section the migration process is essentially split into shrinks and compacts, or grows.
We refer to these two categories as slow and quick migrations henceforth.

The slow migration is conceptually simpler than the quick counterpart: what chiefly happens is that migrating threads compete to lock data pages for themselves and migrate the valid entries therein, i.e.\ those entries that have not been deleted, into the new generation index.
This is done by using the common insertion routine.
After the migration of a page is completed, the thread that previously locked the page sets the page's generation to the new generation.
To improve contention on the acquisition of pages, each thread, identified by the number $i$, starts migrating pages from the $i \mod P_A$ page.
After having visited all pages, it runs a second visit to check if all of the pages' generations have been set to the new generation.
If it does observe that, then it signals to the other accessors that the node migration phase is complete.

A call to the \texttt{compact} method triggers a compact migration, and such is the only mean to trigger a compact migration, but then, if the new generation meta is still not compact, further grow migrations are triggered, until a generation is reached s.t.\ the resulting meta is compact.


\subsection{Quick Migrations}\label{subsec:quick-migrations}
During the quick migration instead, a participating thread sets a local ``participation'' flag to 1 when joining the migration process (initially set to 0 by the leader for all accessors).
The node migration phase is then very similar to the one described in~\cite[\S5.3.1]{maier}.

A global counter is kept in the current generation metadata that specifies the next node to migrate.
A thread runs an atomic fetch-and-add operation to reserve a certain block, i.e.\ a portion of index to migrate.
By default the block size is 4096 index slots, consistent with the implementation of~\cite{maier}.
So that it is possible for a single thread to perform the entire migration process, if the current index size is sufficiently small.
In fact, the additional contention generated for smaller block sizes is not worth the blocking of the other threads.
After a block is reserved, the thread that made such reservation is free to not use atomic operations to migrate the nodes involved, because it effectively reserved both a portion in the current generation index, and in the new generation index.

Consider a block of size 4096 in the current generation index that starts at position $a$, i.e.\ $[a, a + 4096]$.
By~\cite[Lemma~5.1]{maier}, we know that the involved nodes must be migrated within the $[2a, 2(a + 4096 + 1)]$ block of the new generation index.
That is, grow migrations are only allowed to use a growth factor $\gamma = 2$.
By the semantics of the atomic fetch-and-add operation employed, it is known that $a$ is distinct among all participating threads, therefore the blocks in the current generation are distinct, as well as their counter parts in the new generation.
Thus, there is no need for further synchronization between migrating threads than the fetch-and-add operation.

The migrating thread, having reserved a block, clears the relevant portion of the new generation index and proceeds with inserting the same nodes into the new generation index (i.e.\ nodes pointing to the same location in the data table, albeit possibly differently sized and at different positions), skipping over tombstone nodes, and trying to maintain the Robin Hood invariants, falling back to linear probing if that is not possible.
Note that there not being a necessity to use 16-byte aligned CAS operations, the Robin Hood invariants are maintained to a larger extent, compared to the normal insertion routine.
After being done, the participating thread hits a barrier to synchronize the migration phase with the other threads.

After the node migration completes, the leader thread, swaps out the current metadata for the new one and signals to the other threads that the migration has finished.
This applies for both the quick and slow migrations.


\section{Approximate size}\label{sec:approximate-size}

An approximation of the number of elements stored in the hash table is inferred by the number of pages present.
Three separate, atomic counters are kept:

\begin{itemize}
	\item the greatest allocated page $P_A$;
	\item the greatest deleted page $P_D$; and
	\item the greatest refilled page $P_R$.
\end{itemize}

The $P_A$ counter is also used to keep track of the used portion of the pages array, and is the one used when a thread makes a new reservation into the data table; possibly, triggering the allocation of a new page and increasing $P_A$.

The three counters serve to provide a rough estimate on the number of elements currently stored: as new elements are inserted new data table pages need to be allocated, and when numerous elements are deleted they get defragmented into their own pages, before the pages get eventually freed, or be refilled.
Note that initially, when there are yet to be inserted any elements, their respective values will be 0, -1, and -1.

So an upper bound for the number of elements contained in the dictionary is:
\[
(P_A - P_D + P_R) \times |P|
\]

The bound is then also refined by visiting the $P_A$ and $P_R$ pages, decrementing it of the amount of entries that weren't filled in those pages.
There is also a lower bound, which is computed by decrementing the upper bound by the unused reservations, accounted by traversing the accessors' storages.
Therefore, the running time complexity of this operation is $\Theta(t)$.

The mean (floor division by 2) of the lower and upper bounds is then reported to the user upon request.

This routine is also employed by the insert and delete routines to decide when to trigger migrations.


\paragraph{Linearization and lock-freedom.}
This is trivially a lock-free algorithm.
The linearization here depends on the three reads required for $P_A$, $P_D$, and $P_R$, as well as the $\Theta(t)$ reads required to traverse the accessors' storages.

???

\section{Consistent size}\label{sec:consistent-size}

Given that the linearization of the above approximation of the size of the hash table is sub-optimal at best, it was decided that there would also be a definite, sequentially consistent computation of the hash table size.
This is achieved by using thread-local size counters: each thread keeps track of how many items it has inserted, incrementing its local counter, and how many items it has deleted, decrementing its local counter.
(Thus, it may be that one thread's local counter is negative.)
The thread-local counters are kept in each thread's accessor's storage, and are recorded by using regular reads and writes: they are protected by the accessor's lock.

Upon request, a Synchronous Operation (\S\ref{subsec:synchronous-operations}) to safely sum the thread-local counters begins.
The local counters are reset to 0 before the operation ends.
The result is cached into the \texttt{AtomicDict} and a dirty flag is reset in order to avoid restarting a Synchronous Operation at every call, if no insertions or deletions marked the cached value as dirty.
The cached value is stored with a Python integer.
Upon a new request, if the dirty flag is set, the sum of the thread-local counters and the cached value is computed and stored.

Note that it is much faster to keep thread-local counters and then acquire locks to read them, rather than to keep a shared counter.
Using a Synchronous Operation for this also implies that its semantics are trivially sequentially consistent.
It can be argued that to retrieve this consistent size, there is a missed opportunity cost related to the exclusion of all other threads from using the hash table.
While true in principle, it should be noted that this is a $\Theta(t)$ operation, thus independent of the table size, and generally quite fast.
For these reasons, this is the routine that is exposed by default, i.e.\ when calling \texttt{{len(AtomicDict(\ldots))}}.
If a program cannot withstand the added cost of mutual exclusion (perhaps because it frequently asks for the size of the hash table while this is being concurrently mutated), it is still possible to access the approximate size described in the above Section, which is quite accurate and lock-free.
Additionally, under the situation of high-frequency concurrent mutations, it may make more sense to retrieve an approximation of the size rather than a sequentially consistent size.
The sequentially consistent size computed during a call to this routine, may be invalidated immediately upon returning its output.

Notwithstanding the limitation on the number of nodes that can be present into the described hash table at any given time ($2^{56}$), as prescribed by Table~\ref{tab:robin-hood-nodes}, a counter of 64-bits is not necessarily sufficient.
Consider the situation where two threads concurrently insert and delete $2^{64}$ items, so that one thread only performs insertions, and the other only performs deletions (or any other equivalent situation).
Care then needs to be taken in order to avoid running into integer overflows or underflows, a notorious problem in C code.
Before such an event occurs, the thread that detects it (its local counter is \texttt{MAX\_INT64} during an insert, or \texttt{MIN\_INT64} during a delete), the thread repeatedly tries to add (CAS) its local counter to the \texttt{AtomicDict}'s cached value, while holding its local accessor's storage mutex.
This way, no Synchronous Operation can begin, and it is safe to add the protected value, before resetting it to 0.
If another thread finds itself in the same condition, the competing threads will serialize based on the CAS to the cached counter.

\paragraph{Linearization and non-lock-freedom.}
With this being a Synchronous Operation, it is trivial to see that it is not lock-free, and its linearization follows that of other Synchronous Operations.

\section{Batch lookup}\label{sec:batch-lookup}

Inspired by the design of DRAM-HiT, this method was added so as to amortize the cost of multiple memory accesses when a program wants to read a number of keys from the hash table.
It deviates from~\cite{dramhit} in that it does not require the user to continuously poll the table for results.
Instead, a batch of keys can be submitted for lookup, in one invocation.

An example call:

\begin{lstlisting}[language=Python,label={lst:batch-lookup-usage}]
foo = AtomicDict({'a': 1, 'b': 2, 'c': 3})
result = foo.batch_getitem({
  'a': None,
  'b': None,
  'f': None,
})
assert result == {
  'a': 1,
  'b': 2,
  'f': cereggii.NOT_FOUND,
}
\end{lstlisting}

The \texttt{cereggii.NOT\_FOUND} object is a special, global object that cannot be inserted into \texttt{AtomicDict} (as previously described in~\S\ref{sec:insert-or-update}).
The fact that it is in the output means, as its name implies, that the key was not in the dictionary.

The operation involves prefetching each distance-0 location of the keys in the batch.
This results in an amortization of the cost of individual memory accesses: if the number of prefetches issued is sufficiently large (how large depends on the hardware), then the processor will be stalled waiting for the first read, while the subsequent ones will be read from the cache without incurring in the cost of main memory access.
This behavior is cheaper-by-comparison: it is cheaper to prefetch several keys, possibly blocking at the first one read, than to block at every read.

NUMA architectures will enjoy this mechanism particularly: when prefetches are issued at remote memory addresses, the comparative cost of the amortized reads is much less than the sum of the costs of each individual read.

Concerning NUMA architectures, it is generally also possible to argue that the cost of a remote access may be hidden by the possibly many local memory accesses that precede it.
Such is not our case: the index, which is allocated as a contiguous array, will reside entirely in \emph{one} NUMA node.
Thus, a thread accessing the index will either only issue operations to remote memory, or to local memory.
NUMA support is thus limited: the presence of a single shared index makes it impossible to efficiently make use of memory partitioning.

The submitted batch is subdivided into chunks of a configurable size.
This is intended to prevent memory over-fetching.
That is, the cache of an individual processor may become overly filled with the prefetched portions of the index, and the hardware will proceed to evict lines of cache before they are actually read by the lookups that requested them.

\paragraph{\texttt{AtomicShardedDict}.}
A further endeavor may be carried out so as to provide an \texttt{AtomicShardedDict} class which creates its own pool of threads, bound to individual NUMA nodes, and affords access to the hash table shards to those threads only.
The partitioning of a hash table into several shards, each of which is managed by one or multiple threads associated to a single NUMA node, requires that any given key must be uniquely assigned to one shard (e.g.\ by using the least-significant bits of the hash to associate a key with a shard).
Mimicking the design of DRAM-HiT, the user thread would submit requests to a message queue which is consumed by the threads associated with the relevant shard.
This is resembling of the design of~\cite{dramhit}, though this design need not be asynchronous as in~\cite{dramhit} in order to gain the speed-up of stronger memory associativity.
It can be implemented by making use of the current implementation of \texttt{AtomicDict}, such that one shard of \texttt{AtomicShardedDict} is an instance of \texttt{AtomicDict}.


\subsection{Linearization}\label{subsec:batch-lookup-linearization}

The linearization of this method is intended as the individual linearizations of the distinct lookup operations.
Each distinct lookup is linearizable, although the overall invocation is not.
Therefore, it is possible for a result of this method to not be sequentially consistent.


\subsection{Lock-freedom}\label{subsec:batch-lookup-lock-freedom}


\section{Aggregate}\label{sec:aggregate}

A notorious behavior of using CAS semantics is to repeatedly make the call to the CAS routine in order to cope with concurrent mutations to the contended area of memory.
We are especially interested into efficiently and ergonomically handle the mutation of the value of a given key, for such is an important use-case for hash tables in general.

It is proposed here, without an accompanying implementation, an interface that is believed to satisfy the both the efficiency and ergonomics goals, which is also very similar to~\cite[Algorithm~1]{maier}.
Instead of providing an expected and a desired value, the programmer specifies the desired mutation, i.e.\ a function over the current value.
Consider the following piece of Python code.
\begin{lstlisting}[label={lst:aggregate-usage}, language=Python]
	d = AtomicDict({...})

	d.aggregate(key="spam",
	  mutation=lambda current_value:
	    1 if current_value is NOT_FOUND else current_value + 1
	)
\end{lstlisting}
Do note the \texttt{lambda} expression on line 4, which is a Python idiom to define an anonymous function, within an expression.

A key may be already present or not; thus, a programmer using this method should be instructed to write a function that handles both cases, and that there should be no assumptions as to the number of times this function is called before its described mutation is actually applied to the value.
That is, it may be that the function is called multiple times, due to failed CASes (i.e.\ due to contention).

That function is called, possibly multiple times, by a routine that behaves like the following:
\begin{lstlisting}[label={lst:aggregate}, language=Python]
	expected = d.get(key, cereggii.NOT_FOUND)
	while True:
	  try:
	    d.compare_and_set(key, expected, mutation(expected))
	  except cereggii.ExpectationFailed:
	    expected = d.get(key, cereggii.NOT_FOUND)
	  else:
	    break
\end{lstlisting}
Such routine would likely be implemented in C code, for the sake of uniformity with the rest of the implementation of \texttt{AtomicDict}.

The specific method of calling CAS in the above Listing~\ref{sec:aggregate} is very commonly known in the literature, albeit certainly more common to be written in a form where the CAS operation returns a boolean, rather than raising an exception.
(See also the relevant discussion in~\S\ref{sec:insert-or-update}.)
The fact that this is well known in the literature, though, does not directly imply that it is also well known in the literacy of Python programmers.
For many programmers who are not accustomed to the domain of concurrency, the above Listing should probably be explained in detail, before its semantics are appreciated.
Providing a transparent and standardized way to access such common knowledge is therefore considered fruitful.

Do note that the atomicity of this \texttt{aggregate} method is violated if within the provided mutation function, a lookup into \texttt{d} is performed.
Or in other words, this is not a suitable design for a mutation that requires a sequentially consistent view of more than one key in the dictionary.

\subsection{Reduce}\label{subsec:reduce}

In order to provide further performance improvements, another method could be added, already otherwise known in computing as \emph{reduce}.
Such is a foundational concept of many computing models, most notably those based around the concept of map-reduce like for instance Hadoop.
As such, it could easily be provided based on the above \texttt{aggregate} method, and further enhanced to minimize contentious access to the dictionary.
Its usage would be something resembling the following lines of code:
\begin{lstlisting}[label={lst:reduce-usage}, language=Python]
	d = AtomicDict({...})

	batch = [
	  (key_1, value_1),
	  ...
	  (key_n, value_n),
	]

	d.reduce(iterator=batch,
	  mutation=lambda key, current_value, new_value:
	    new_value if current_value is NOT_FOUND
	    else current_value + new_value
	)
\end{lstlisting}

The mutation function here is made more complex because it needs to be also independent of the key and value: the same function needs to be applied for every key-value pair in \texttt{iterator}.
The parameter \texttt{key} is added so as to satisfy the possible cases in which the computation is dependent on the key itself.
It is immediately clear for a customary Python programmer that the \texttt{lambda} expression could be exchanged for a full, non-inline function definition using \texttt{def}.
Such is the expected usage for more complex mutations than the trivial one shown here.
Nevertheless, it is worth pointing this out for the less Python-expert reader.

Contention can be reduced by first accumulating an intermediate result into a thread-local hash table, and then applying the \texttt{aggregate} method for each item in the local hash table.
This reduces contention because the repetition of keys in the input is not made visible to other threads, but only to the thread-local hash table.

Since this thread-local hash table certainly requires allocation of memory, its size could also be bound.
Upon reaching the bound the accumulated intermediate result is applied to the \texttt{AtomicDict}, and then the operation is repeated until the exhaustion of \texttt{iterator}.


\paragraph{Linearization and lock-freedom.}
The linearization and lock-freedom of \texttt{aggregate} and \texttt{reduce} is chiefly dependent on the respective properties of the \texttt{ExpectedInsertOrUpdate} routine that is being called in order to implement them.
Thus, refer to~\S\ref{subsec:insert-linearization} and \S\ref{subsec:insert-lock-freedom} for further discussion.


\section{Iterations}\label{sec:iterations}

Iterations are designed to be particularly efficient in \texttt{AtomicDict}, mostly because of the design of the data table, which helps with cache efficiency, since the entries are close to one another.
The hardware memory pre-fetching subsystem can also help by observing the linear memory access pattern.

(In~\cite{maier}, iterations are referred to as \emph{Forall} operations.)

In fact, the presented iteration system, accessible through the \texttt{{fast\_iter}} method, has been showed to perform twice as fast, compared to Python's \texttt{dict} iterator, even for single threaded executions (see Chapter~\S\ref{ch:measurements} for measures).
Additionally, the partitioning scheme described in the following Section enables truly multi-threaded usage of an iterator, by creating per-partition iterators.
This is something that is not available in \texttt{dict}'s implementation.

Iterating is done primarily over the data table.
A thread starts by fetching the first page, then (skipping over empty or deleted cells, including entry 0) reads the first entry of the page, yielding the item back to the caller (generally i.e.\ Python code), after storing the next location to visit.
The Python code then loops calling this same function again, which will then yield the result of the stored location, and increment the next location to visit, before yielding again.
This requires the creation of a small iterator \texttt{PyObject} to keep track of the next location to visit, a necessity for adhering to the Python loop protocol regardless.


\subsection{Iteration Partitioning}\label{subsec:iteration-partitioning}

The so-far described iteration scheme is further enhanced with partitioning.
When multiple threads want to iterate over the same hash table, it may very well make sense for the application that they visit disjoint subsets of the stored key-set.

This is achieved by letting the thread declare the number of partitions $\pi$, and the partition number $i$ assigned to it (a thread identifier of some sort), s.t. $i \in [0, \pi)$.
In order for the partition to actually yield disjoint subsets, $\pi$ has to be known to the other participating threads as well, and is intended to be equal to the number of threads participating in the iteration.
Furthermore, $i$ needs to be assigned uniquely to each thread.

A call to \texttt{{fast\_iter(partitions=$\pi$, this\_partition=$i$)}} will then visit those pages for which it holds that their page number $p \mod \pi = i$.
Therefore, the number of keys per subset is not guaranteed to be to be uniformly distributed.
Nevertheless, if the key-set is large enough (that is, $|K| \gg \pi \times |P|$), and the number of deleted keys per-page is nearly constant on average (that is, let $\mu = E[d(p)]$ and $\sigma$ its standard deviation, $\sigma \ll \mu$), then the distribution of keys per partition is also nearly constant on average.


\subsection{Non-linearization and lock-freedom}\label{subsec:iteration-linearization-lock-freedom}
While the lock-freedom property this operation enjoys is trivial, its linearization is trickier.

This iteration is in fact not guaranteed to be consistent with any sequentially consistent access, and is therefore not considered linearizable.
In fact, repeated calls to ask for the next element in the iterator are required in order to fulfill the iteration completely, as per Python's iteration protocol.
As such, it cannot be that, as informally defined in~\cite[Principle~3.5.1]{art-mp}:
\begin{quote}
	[the] method call [appears] to take effect instantaneously at some moment between its invocation and response.
\end{quote}
Trivially then, the formal definition~\cite[Definition~3.6.1]{art-mp} also does not apply.

It is therefore advised to use this faster iterator when it is somehow known, by the logic of the program, that there will be no concurrent mutations.
Otherwise, the following sequentially inconsistent behaviors might be observed:
\begin{enumerate}
	\item the same key is emitted more than once; or
	\item an update $u_1$ that happened strictly before another $u_2$ is not seen, while the latter is.
	(Such is the case when two updates are executed in succession by the same thread.
	Thus, it is definitely known that $u_1 \rightarrow u_2$.)
\end{enumerate}

These may be very surprising behaviors for a programmer trying to debug an incorrect execution of an iterating thread.

Those behaviors may be exhibited in the following circumstances, respectively:
\begin{enumerate}
	\item the iterating thread $t_l$ visits the data table entry $e$ in which some key $k$ is stored $\rightarrow$ another thread $t_d$ deletes $k$ $\rightarrow$ another thread $t_i$ re-inserts $k$ at a location $e'$ in the data table s.t.\ $e < e'$ $\rightarrow$ $t_l$ visits $e'$ in which $k$ is stored.
	(It may also be that $t_d \equiv t_i$.)
	\item the iterating thread $t_l$ visits the data table at location $e$ where the key $k_1$ is stored $\rightarrow$ update $u_1$ to key $k_1$ is performed by another thread $t_{u_1}$ $\rightarrow$ update $u_2$ to key $k_2$, stored at location $e'$ s.t. $e < e'$ is performed by another thread $t_{u_2}$ $\rightarrow$ $t_l$ visits the data table location $e'$, seeing the effect of update $u_2$.
	(It may also be that $t_d \equiv t_i$.)
\end{enumerate}


\section{Proposal for a consistent iteration}\label{sec:consistent-iteration}

The following proposal for a consistent iteration has not been implemented yet, but it should be fully described here.
Nevertheless, once implemented, it will be the standard iteration scheme adopted, given that its semantics are much more immediate to understand.

When a thread iterates over the hash table with the iterator described above in~\S\ref{sec:iterations}, it may encounter sequentially inconsistent behaviors, if other threads are concurrently mutating the table.
The necessity for a sequentially consistent iterator lies therein.

To implement a cost-efficient and consistent iteration seems to be possible, based on the following design.

\begin{enumerate}
	\item The iterating thread $t$ begins a Synchronous Operation (\S\ref{subsec:synchronous-operations});
	\item it generates an iteration ID $i$ (given that there will be need to store some data per iterator, it may be the pointer to this C structure);
	\item it allocates a new local pages array of the same size of the used portion of the data table's pages array;
	\item it allocates a new local index and copies the shared index into it;
	\item it traverses the \texttt{AtomicDict}'s pages array, and for each page:
	\begin{enumerate}
		\item it sets the \texttt{iteration} field on the page to $i$;
		\item it copies the page pointer into the local pages array;
		\item it increments the page reference count;
	\end{enumerate}
	\item $t$ ends the Synchronous Operation; and
	\item proceeds iterating over the new pages, in a manner similar to the iteration described in~\S\ref{sec:iterations}.
	\item When the iteration completes, it decrements the reference count of the pages in the local pages array, and frees the array.
\end{enumerate}

When the iteration field of a page is set (i.e.\ $\neq$ \texttt{NULL}), the page is considered to be immutable.
A thread that wishes to mutate it, should thus first copy the page's contents into a newly allocated page, and then atomically swap the two pages in the pages array.
If the swap fails, then it means that another thread has swapped the page into a mutable page: it cannot be that the newly swapped-in page is immutable because that requires holding all of the accessors' locks.

The creation of new mutable pages is therefore entirely offloaded, from the iterating thread, to the other \texttt{AtomicDict}'s accessors.
This is a feature, rather than a compromise, of this design.
Consider the opposite case in which the iterating thread additionally has to allocate more pages and swap them in the pages array, or equivalently has to create its own immutable pages.
In this scenario the Synchronous Operation itself would take much longer.
Thus, hindering the performance of the other accessors which are trying to apply mutations to the hash table (recall that lookups are permitted in spite of Synchronous Operations).
That is, there is a missed opportunity cost associated with it.

Instead, the parallelization of threads which independently swap the immutable pages out for their mutable copies, entails that multiple threads can be used to create the new pages.
And, furthermore, that if no mutations were actually going to happen, either at all or in a subset of the pages, then strictly less work is required overall.

It may also be entirely possible to permit the re-usage of the iteration ID\@.
So that if a thread sees that all pages refer to the same iteration ID, then it avoids marking the pages as immutable, and instead copies the associated data into its own iterator.


\paragraph{Linearization and non-lock-freedom.}
With this being a Synchronous Operation, it is trivially not lock-free.
Its linearization also follows the semantics of the Synchronous Operation: the iterator linearizes when it manages to acquire all the required locks.
