\chapter{Implementation}\label{ch:implementation}

The implementation is available online at~\cite[src/cereggii/atomic\_dict]{cereggii}, and comprises 12 C source files, with Python bindings, and 29 test cases (written in Python code).

\cite{peniocereus-greggii}

$O(t)$

$\textrm{atw} \textrm{rgw} \textrm{rgr}$

The bindings to the Python runtime are considered an implementation detail, and are not discussed here.
They chiefly consist of mapping Python objects into C constructs, which is achieved with calls to Python's C-APIs.
Another feature of Python's C-APIs that has been extensively used is garbage collection, which is discussed in \S\ref{sec:garbage-collection}.


\section{Lookup}\label{sec:lookup}

The proposed lookup design is what could probably be expected given the discussed design of the hash table.
It departs sharply from Maier's, as it should be expected, and, in fact, it is not exactly the same as Python's either, due to the employment of Robin Hood hashing, and lack of key-sharing.
A fairly complete C-like pseudocode of its implementation is reported in Listing~\ref{lst:lookup}.
The operation begins by computing the $d_0$ index slot, based on the pre-computed hash, by calling the \texttt{Distance0Of} function, which internally re-hashes the Python-generated hash, as described in~\ref{subsec:maier}, and then computes the slot position, based on the size of the hash table, listed in the \texttt{meta} object.
The hash computation is deliberately omitted from Listing~\ref{lst:lookup}, because it simply entails making the correct call to the relevant Python C-API, and checking for possible errors.

The algorithm then looks up the index for the searched element, starting from position $d_0$, until either:
\begin{enumerate}
    \item the item is found;
    \item an empty slot is found (the only probe in which the item could be found was looked at in its entirety);
    \item the hash table is in a compact state (i.e.\ no non-compact nodes are in the index) and the search can be pruned due to the Robin Hood invariant (i.e.\ the distance of the read node is greater than what the distance of the searched node would be if it was to be found); or
    \item the probe distance is equal to the index size.
\end{enumerate}

The searched key can be found if
\begin{enumerate}
    \item there is a node in the index whose tag corresponds to the key's hash;
    \item the node points to an entry other than 0;
    \item the entry's value is not \texttt{NULL};
    \item the stored (Python-generated) hash is not different from the key's hash; and either
    \begin{enumerate}
        \item the pointers of the searched key and the stored key are the same (identity); or
        \item the keys are semantically equal, based on the arbitrary logic with which those two Python objects may be deemed equal.
    \end{enumerate}
\end{enumerate}

Comparisons in Python are notoriously costly.
As is well known, Timsort was implemented specifically for Python, with the explicit goal of minimizing the amount of comparisons as much as possible.
It can be seen that in lines 38--55, care is taken in order to delay the call to the comparison function as much as possible.
This is because comparisons between arbitrary Python objects can be very computationally expensive.
Since the involved object's types may be implemented in Python code, it can require interpreting an arbitrary Python function.
Such function does not have any restrictions on its computational complexity, and even if it was enforced to be an $O(1)$ operation (as it would be normally expected), its constant factor would still be very high, due to interpretation overhead.
The comparison itself may also raise an exception, either if the two objects' types are not comparable, or if the arbitrary Python code that was executed to compare the two objects raised an exception for any reason (a circumstance handled in line 57).

Do note that inside the \texttt{ReadEntry} sub-routine the key is always read before the value.
This avoids the problem of torn-reads, exactly as described in~\cite[\S4, Lookup]{maier}.

On line 69 the eventuality that the compact state of the hash table has changed is taken into account.
Do note that for a generation of the hash table the only transition that can occur to the compact state is from $\top$ to $\bot$.
In order to restore the compact state (from $\bot$ to $\top$), a migration is required.
(The concept of \emph{generation} is introduced in the Migrations Section~\ref{sec:migrations}.)
Suppose the check on line 69 didn't exist, and consider two threads $t_l$ and $t_i$ executing respectively a lookup for key $k$ and an insert for the same key.
There can exist an execution s.t.:
\begin{enumerate}
    \item $t_l$ runs first and reaches line 8, reading that \texttt{is\_compact} $= \top$;
    \item $t_i$ runs from start to end, turning \texttt{{meta->is\_compact}} $= \bot$, and inserting $k$ with a non-compact node; and
    \item $t_l$ runs again, not finding the node pertaining to $k$, thus returning a lookup failure.
\end{enumerate}
Instead of this evidently faulty behavior, the check on line 69 makes the lookup restart from the beginning, so that item (3) above would be substituted with
\begin{enumerate}
    \item[3.] $t_l$ at first not finding $k$, sees that a non-compact insertion could have linearized, undermining its assumption that the Robin Hood invariant holds, and thus restarts from line 7, eventually finding $k$.
\end{enumerate}

Another similar check can be found on lines 72 and 79, where concurrent migrations are considered.
It is important to note that lookups are permitted to run concurrently with migrations, as they are read-only operations.
When a concurrent migration from the current generation of \texttt{meta} to a newer generation has been linearized (setting this metadata's \texttt{{migration\_done}} flag to $\top$) it is possible that a subsequent insert for the same key as the lookup operation linearizes before the lookup completes, in a manner similar to the previously discussed compactness of the hash table.
In such scenarios, the lookup operation is restarted.
In this case, the $d_0$ position of the key must also be recomputed.

Towards the end of Listing~\ref{lst:lookup}, at lines 68--94, is where the response is written into an externally allocated C \texttt{struct}.
Given the fact that this routine is also employed elsewhere (see Listing~\ref{lst:delete}, line 8), it would be insufficient to return a simple boolean value.
Instead the following is returned, which completely describes the state of the relevant memory pertaining to the search key:
\begin{enumerate}
    \item an \texttt{error} flag, if $\top$, the rest of the values returned are undefined;
    \item a \texttt{found} flag, if $\bot$, the returned values below are undefined;
    \item an integer \texttt{position}, indicating at which slot of the index the relevant node was found;
    \item an \texttt{{is\_compact}} flag, indicating whether the node was found to be compact in the index;
    \item a \texttt{node} structure, with the node that was found;
    \item an \texttt{{entry\_p}} pointer, to the relevant data table entry; and
    \item an \texttt{entry} structure, with the values read from the data table entry.
\end{enumerate}

\lstinputlisting[
    numbers=left,
    stepnumber=5,
    numberfirstline=true,
    language={C},
    caption={Lookup Operation},
    label={lst:lookup}
]{listings/lookup.c}

\subsection{Linearization}\label{subsec:lookup-linearization}

A successful lookup linearizes on line 41, where the key and value are read.
Failed lookups, i.e.\ lookups that don't find the searched key, linearize on line 15, where a node is read from the index, be it an empty node, or a node $n$ whose distance-0 position $d_0(n) >$ distance-0 position of the searched key.

\paragraph{Lock-freedom.}

It is trivial to see that no thread's arbitrary delay can induce this lookup to also be subjected to an arbitrary delay, insofar as the code presented is concerned.
Thus, the lookup operation is lock-free.

It is true that there is one aspect that evades the control of the author, namely the equality comparison of line 55.
It can indeed be that the arbitrary code that is run also comprises a lock acquisition.
Lock-freedom of that relevant code can in no way be evinced, nor enforced.
Therefore, we can only assume (or rather, hope) that the arbitrary code is sane and does not exhibit such surprising behavior.

The \texttt{Lookup} above described is not wait-free.
In fact, the check at line 69 makes it so that when a concurrent insert sets the metadata's \texttt{{is\_compact}} flag to $\bot$, and the similar check for migrations on line 72, the lookup operation needs to be retried.


\section{Insert or Update}\label{sec:insert-or-update}

The \texttt{ExpectedInsertOrUpdate} routine shown in Listing~\ref{lst:insert} departs from Maier's design of~\cite[Algorithm~1]{maier} in that it doesn't expect the user to provide an update function, but rather mimics more the compare and set routines of other atomic data structures: it takes as input the expected and desired values, and it returns the value that was stored before the update was performed (or a special \texttt{{NOT\_FOUND}} object).
If the expected value was not the stored value (or the compare and set low-level call failed and the newly read value then differed from the expected one), then a special \texttt{{EXPECTATION\_FAILED}} object is returned.
A routine that more closely resembles Maier's Algorithm~1, is described later in~\S\ref{sec:reduce}, based on the one discussed here.

There's also another special value called \texttt{ANY} that can be used as the expected value to signify that whatever the current value is, even an absent value, it should be replaced with the desired value.
Returning the previous value, in conjunction with \texttt{ANY} as the expected value, also serves to reuse the same routine to behave like a swap, rather than a CAS\@.

The three special objects \texttt{{ANY}}, \texttt{{NOT\_FOUND}}, and \texttt{{EXPECTATION\_FAILED}} cannot be used as keys or values in \texttt{AtomicDict}, so as to guarantee their semantics are consistent (cf.\texttt{\ {EXPECTATION\_FAILED}} being a value returned by this routine).
This is enforced, but not otherwise shown here.

If the expected value is either \texttt{{ANY}} or \texttt{{NOT\_FOUND}}, there is an insertion fast-path used in which the $d_0$ element in the index is CASed with the entry reserved without going through the currently stored data in the index.
If this CAS succeeds, then the operation is completed.
If it doesn't the general code path follows.
Notice that this doesn't compromise correctness: if the $d_0$ node was in fact empty, it means that the key wasn't in the hash table.

Here, it is worth mentioning that this routine is not directly exposed, and is instead considered internal.
Instead of calling this function directly, a user of \texttt{AtomicDict} is expected to call different Python idioms, which all internally call \texttt{ExpectedInsertOrUpdate}; namely:

\begin{enumerate}
    \item \texttt{{d[k] = v}}, eq. \texttt{{ExpectedInsertOrUpdate(\ldots, k, \ldots, ANY, v, \ldots)}};
    \item \texttt{{d.compare\_and\_set(k, exp, des)}}, eq. \texttt{{ExpectedInsertOrUpdate(\ldots, k, \ldots, exp, des, \ldots)}}, which raises an exception when the expected value was not the stored value;
    \item \texttt{{d.swap(k, v)}}, eq. \texttt{{ExpectedInsertOrUpdate(\ldots, k, \ldots, ANY, v, \ldots)}}, which is similar to (1) except it also exposes the previous value.
    (Not yet implemented.)
\end{enumerate}

In regards to (2) above, returning a boolean value is generally considered the standard.
Such is what the hardware primitive essentially does, and also what other languages such as Java do.
Nevertheless, in order to favor the usage of the Aggregate routine of~\S\ref{sec:reduce}, which correctly implements a generic mutation to a value, it was chosen to expose this behavior instead.
That is, simpler usages, such as incrementing a counter, or any other ``aggregations,'' should instead employ the more appropriate \texttt{aggregate} method.
This \texttt{compare\_and\_set} method is thus intended to be used in scenarios where the expectation is ``taken for granted,'' and would otherwise require debugging if unmet; that is, where the failure of the expectation has more severe impacts than mere contentious access.
Do note that this design does not pertain to any correctness or performance goal; it, instead, has to do with the programmer's ergonomics in the usage of this lower-level routine.
Not every programmer possesses thorough knowledge of concurrency issues, and an interface that alleviates some of those complexities is what the author strives to achieve.
Whether or not this is actually simpler to use remains to be seen.

The \texttt{ExpectedInsertOrUpdateCloseToDistance0} sub-routine, called at line 35, is omitted for brevity.
What it does can be described with the following steps:
\begin{enumerate}
    \item consider the nodes in the 16-byte-aligned region of memory the $d_0$ slot is part of, in sequence, starting from $d_0$:
    \begin{enumerate}
        \item if an empty slot is found either set \texttt{expectation} $=\bot$, if \texttt{expected} is neither \texttt{ANY} nor \texttt{{NOT\_FOUND}}, or attempt to insert the node in keeping with the Robin Hood invariant;
        \item if a non-empty slot is found, check the tag, and possibly call the \texttt{ExpectedUpdateEntry} routine;
    \end{enumerate}
    \item set the \texttt{{must\_grow}} flag if the node would be inserted at a distance greater than the current maximum distance (and avoid modifying the index);
    \item set the \texttt{done} flag accordingly.
\end{enumerate}

The \texttt{ExpectedUpdateEntry} sub-routine is also omitted for brevity; its implementation resembles the \texttt{{check\_entry}} part of the \texttt{Lookup} routine (Listing~\ref{lst:lookup}, lines 38--65), but also performing an update, possibly signaling that the expectation failed.


\lstinputlisting[
    numbers=left,
    stepnumber=5,
    numberfirstline=true,
    language={C},
    caption={ExpectedInsertOrUpdate Operation},
    label={lst:insert}
]{listings/insert.c}

\subsection{Linearization}\label{subsec:insert-linearization}

The presented \texttt{ExpectedInsertOrUpdate} operation is linearizable.

A successful insertion linearizes upon the insertion of the new node into the index (note that a data table entry for an insertion is prepared beforehand, and possibly cleaned up later if deemed unnecessary, that is, if the routine turned into an update).
Before that happens, concurrent lookups and deletes have no visibility on the data table entry which hosts the yet-to-be inserted key, since their search is based on a hash lookup on the index before looking into the data table.
After it happens, either with a successful CAS on line 56 (non-compact) or equivalently with a successful CAS inside the compact sub-routine, concurrent lookups will see the new node, and visit the pointed-to data table entry, successfully finding the key.
(Consider a call to \texttt{ExpectedInsertOrUpdateCloseToDistance0} to essentially boil down to a laborious computation, before picking the right value with which to call CAS\@.
Thus, externally it can be observed to behave in much the same way as a non-compact insertion.)

Pertaining concurrent iterations (described later in Sections~\ref{sec:iterations} and~\ref{sec:consistent-iteration}), which mainly focus on reading the data table, the linearization point thus described still holds.
When an iteration visits an entry, it additionally checks for the relevant node in the index to be present and if it is not, then it avoids yielding that value.

\subsection{Lock-freedom}\label{subsec:insert-lock-freedom}

The described \texttt{ExpectedInsertOrUpdate} is lock-free, inasmuch as it does not trigger a grow migration.

As mentioned earlier, we maintain the lock-freedom property only for those operations that don't necessitate migrations to complete.
As described, this is because the implementation of completely lock-free hash tables is deemed impractical.
Where migrations are not concerned, this routine is indeed lock-free: no thread's arbitrary delay can arbitrarily delay this routine as well.
Later, we will not specify this caveat of lock-freedom holding only when no migrations are in progress; such is also sustained by Maier et al.\ in~\cite{maier}.

\texttt{ExpectedInsertOrUpdate} is not wait-free.
An insert may in fact be delayed in several points:
\begin{enumerate}
    \item when making a reservation in the data table (which is not shown, for brevity, in Listing~\ref{lst:insert});
    \item when failing to make use of the insert quick-path of line 25;
    \item when the CAS inside the \texttt{ExpectedInsertOrUpdateCloseToDistance0} sub-routine fails, due to the index nodes involved being contended;
    \item when a non-compact node fails to be inserted on line 56, again due to contention on the index position;
    \item when a concurrent update or deletion changes the already-present key (thus, this would be an update, rather than an insert); and
    \item when the check on line 103--104 fails, due to another thread changing the index state to non-compact.
\end{enumerate}
Note that the delays here mentioned do not cause obstruction for a running \texttt{ExpectedInsertOrUpdate}, which remains lock-free, notwithstanding the contention issues listed above.


\section{Delete}\label{sec:delete}

Albeit an infrequently used feature of hash tables, according to the data shown in~\S\ref{sec:dict-metrics}, supporting deletions has taken a substantial portion of the development time for the implementation of \texttt{AtomicDict}.
Several design iterations have been evaluated, and are not explored here in detail.
The current proposal, which is not fully implemented as of the time of writing, is what follows.

Deleting a key $k$ from the hash table is considered linearized when the value, i.e.\ a pointer to a \texttt{PyObject} stored in the data table entry, associated with $k$ is set to \texttt{NULL}.
Being such a conceptually simple semantics for deletes, this is the part that has remained constant throughout the iterations on the design.

After the delete is linearized, the following housekeeping actions are taken:
\begin{enumerate}
    \item the relevant index node is exchanged for a tombstone, so as to avoid requiring lookups from visiting the data table entry;
    \item a synchronous operation may be started so as to defragment the data table; and
    \item in addition to the above synchronous operation, a shrink migration may be triggered as well, if the estimated size, given by the upper-bound formula in~\S\ref{sec:approximate-size}, is less than the minimum fill ratio (i.e.\ 1/3 by default.).
\end{enumerate}

The three additional steps are carried out in the delete routine, because:
\begin{itemize}
    \item deletes are considered infrequent, so adding cost to them and relieving other routines of this work is considered to be a decrease in cost overall (as is the case for the tombstone exchange); and
    \item if deletes happened to be frequent instead, it would likely be that the program expects the memory consumption of the hash table to decrease (which is what we do by defragmenting the data table and triggering a shrink migration).
\end{itemize}

\subsection{Exchanging the node for a tombstone}\label{subsec:exchanging-node-tombstone}

The swapping for a tombstone node is fairly simple and is shown in lines 33--36 of Listing~\ref{lst:delete}.
Recall that doing this swap instead of simply deleting the node is necessary, as already described in~\S\ref{sec:lazy-robin-hood}, and in~\cite[\S6.4]{the-art-vol-2}.
In our circumstances, it is especially useful in order to help concurrent lookups: the thread looking up a key $k$ may very well visit the data table entries pertaining to the visited nodes in the index, possibly finding that $k$ had been deleted by reading that its associated value $=$ \texttt{NULL}.
But reading a data table entry is much more costly than reading a node in the index, given that the index nodes are much smaller in size compared to a data table entry and that the next node index to visit is very likely to already be in the processor's cache, while the entry is likely not.

The \texttt{LookupEntry} routine, omitted for brevity, is essentially equivalent to the \texttt{Lookup} routine shown in Listing~\ref{lst:lookup}, but instead of looking for a node that points to an entry that contains the searched key, it looks for a node that points to a certain fixed location of the data table; thus it is an operation that only involves reading the index.

The tombstone node is possibly moved further down its probe, if such is possible within a single atomic write; that is, as long as the atomic write is of at most 16-bytes, and is 16-bytes aligned.

\subsection{Defragmenting the data table}\label{subsec:defragmenting}

Keep a per-page deletions counter noted as $\delta(p)$, for some page $p$.
After a delete, atomically increment $\delta(p)$.
If $\delta(p) \geq \frac{|P|}{2}$, initiate a merge.

The following description is what occurs after a merge operation has been triggered.
\begin{enumerate}
    \item Look for another page $p'$, distinct from the page from which the merge was triggered, s.t. $\delta(p') \geq \frac{|P|}{2}$.
    \begin{enumerate}
        \item Since page number 0 cannot be cleared because it hosts entry number 0, which has to remain for the correctness of tombstones, start the search from page number 1 (these checks are omitted for brevity from Listing~\ref{lst:delete}); and
        \item if no such page can be found, abort the merge operation.
    \end{enumerate}
    \item W.l.o.g.\ call $p_1$ the lower-numbered page, and $p_2$ the other.
    \item Let $p_0$ be the lowest non-empty page in the data table.
    It may be that $p_0 \equiv p_1$, and that is fine.
    \item Move the elements found in $p_0$ into $p_2$.
    When $p_2$ becomes full, move the remaining elements into $p_1$ instead.
    (Note that if $p_0 \equiv p_1$, then $|p_0| = |p_1| \leq \frac{|P|}{2}$, and also $|p_2| \leq \frac{|P|}{2}$, therefore $|p_0| + |p_2| \leq |P|$.
    Thus, all elements from $p_0 \equiv p_1$ will be moved into $p_2$.)
    That is, $\forall e \in p_0$:
    \begin{enumerate}
        \item clear $e \in p_0$;
        \item write $e$ into a free slot of $p_2$ (possibly overwriting deleted slots);
        \item if $e$ is a reservation with unoccupied slots left:
        \begin{enumerate}
            \item write the unoccupied slots too;
            \item look into every accessor's storage (this is already a Synchronous Operation) and edit the relevant reservation to point to the new slot; and
        \end{enumerate}
        \item update the relevant node in the index to point to the new location.
    \end{enumerate}
    \item Update $\delta(p_0)$, $\delta(p_1)$, and $\delta(p_2)$ accordingly.
\end{enumerate}

The relevant pseudocode can be found in Listing~\ref{lst:delete}, lines 40--87.
Note that step (4) above is summarized on line 70.

\subsection{Triggering a shrink migration}\label{subsec:triggering-a-shrink}

After a merge operation completes, as described in the above Section, a shrink migration may be triggered, following the increment of the greatest deleted page field in the metadata.
It would cause the size of the index and of the pages array to be reduced, if usage is below a certain threshold, which by default is of 1/3, consistent with Python's \texttt{dict} defaults.

The thread caller of \texttt{Delete} is responsible for actually initiating the shrink migration, after having yielded its accessor's lock.

\subsection{The current implementation}\label{subsec:the-current-implementation}

The current implementation, which is not shown here, is essentially similar to the one described so far, but instead of the page-merge algorithm, defragmentation is carried out by swapping entries.
Which is the implementation that can be found in~\cite{cereggii}, available online at \url{https://github.com/dpdani/cereggii/blob/40eb43084232263716dc5e7fe753253fdc042e6b/src/cereggii/atomic_dict/delete.c}.\footnote{%
    Last accessed \today.
}
When a key is deleted, its entry is swapped with another entry stored at a lower-numbered location, so as to maintain the possibility of eventually incrementing the deleted pages counter ($P_D$ in~\S\ref{sec:approximate-size}).
Notice how the necessity for defragmentation remains, in order to provide a fairly cheap and wait-free approximation of the size, which is required by the \texttt{Delete} and \texttt{ExpectedInsertOrUpdate} routines in order to know when to trigger a migration.
Other mechanisms for triggering migrations that don't require defragmentation could have been adopted, but this one is particularly useful because it also makes iterations faster, by reducing the necessity of an iterating thread to visit empty entries of a fragmented data table.

A way for the swapping operation to be carried out safely in spite of concurrent mutations was found.
At first the deleted entry, already having value $=$ \texttt{NULL}, would be marked as entombed, so that successive unfinished modifications due to the swap mechanism would not be read as valid entries by other threads.
Then, a swapping entry would be looked for in the data table, at a lower-numbered location.
The swapping entry would be flagged as such, so that no other concurrent delete may try to swap the same entry.
Then, the key and value of the swapping entry would be copied to the deleted entry, whose entombed state would be kept.
The value of the swapping entry would be substituted with a flagged pointer to the entombed entry, which would signal to a thread that is reading the swapped entry that it should go and read the other entry.
Finally, the index node pointing to the swapped entry is modified to point to the previously entombed entry.

The above is considered possible to implement with the lock-freedom property.
Nevertheless, it exhibits a problem concerning torn reads: suppose a thread reads the key part of the entry before the entry is marked as deleted, then it gets deleted and swapped, the thread that read the previous key now proceeds and reads the swapped value, thinking that it pertains to the prior key.
However much unlikely this scenario may appear to be, it still needs to be considered, given that the hardware does not generally expose double-word atomic reads.

Swapping entries to defragment unoccupied slots of data table pages, requires a lot of care.
This renders the implementation of deletes much harder to explain and to maintain over time, possibly by people other than the present author.
It also doesn't provide any immediate benefit: memory reclaim is deferred to the next shrink anyway, whereas merges being Synchronous Operations (albeit shorter operations compared to shrinks), means that memory is reclaimed sooner and more frequently.

\lstinputlisting[
    numbers=left,
    stepnumber=5,
    numberfirstline=true,
    language={C},
    caption={Delete Operation},
    label={lst:delete}
]{listings/delete.c}

\subsection{Linearization}\label{subsec:delete-linearization}

The presented \texttt{Delete} operation is linearizable.
Its semantics (i.e.\ removing a key $k$ from the key-set $K$), are observable instantaneously at line 18 when the value associated with $k$ is set to \texttt{NULL}, with an atomic CAS operation.
Such is for a successful invocation, i.e.\ one that actually removes $k$ from $K$.
Whereas a failed invocation, i.e.\ one that finds $k \not\in K$, is linearized inside the call to the \texttt{Lookup} routine of line 8.
With the \texttt{Lookup} routine being already shown linearizable in~\S\ref{subsec:lookup-linearization}, it follows that also a failed invocation of a delete operation is linearizable.

\texttt{Delete} is linearizable also in spite of concurrent deletions of the same key $k$.
Under such circumstances, the successful CAS of line 18 by the one successful invocation linearizes that call to \texttt{Delete}, as above; while the failed CAS of but the same line 18, will result into the visiting of line 22, forcing the delete operation to be retried, and then fail, as aforementioned.

\subsection{Lock-freedom}\label{subsec:delete-lock-freedom}

The presented \texttt{Delete} operation is lock-free, when defragmentations or migrations are not required.
(And when they are, the linearization point happens before the non-lock-free part of deletions begins.)
\texttt{Delete} is not wait-free, because a delete for a certain key $k$ by thread $t_1$ may be delayed by a concurrent delete, for the same key $k$ by another thread $t_2$.
(It may also be delayed by a concurrent update, but the presented reasoning is sufficient to prove the lack of wait-freedom.)
When that happens, $t_1$ is delayed in performing a retry, as specified in the pseudocode at line 22.
If the same key $k$ is continuously deleted and re-inserted by threads other than $t_1$ it may be that $t_1$ is delayed indefinitely, since there is no mechanism for fairness.
It follows that the presented deletion is not wait-free.


\section{Migrations}\label{sec:migrations}

When the hash table becomes too full ($\approx 2/3$ used slots), or too empty ($\approx 1/3$ used slots), a migration is triggered by the insert and delete routines, respectively.
At its core, such is necessary because it is not possible to simply add more slots into an array after it has been allocated; and the index and data table pages array are in fact regular arrays.
Thus, in order to allow for more keys to be inserted, or for memory to be reclaimed, it is necessary to substitute the aforementioned arrays.

\texttt{AtomicDict} allows for three kinds of migrations: \emph{grow}, \emph{shrink} and \emph{compact}.
Respectively, they allow:
\begin{enumerate}
    \item to double the maximum possible size of the key-set,
    \item to halve it, and
    \item in contrast with the other two, to recover the Robin Hood invariants, which may have been lost due to our Lazy Robin Hood approach, described in~\S\ref{sec:lazy-robin-hood}.
\end{enumerate}
The three kinds are slightly different from each other in their implementation, but are externally signalled in the same manner: by setting a field in the current metadata to show that there is a migration in progress.

Migrations are executed entirely or in part within a Synchronous Operation.
The application of a Synchronous Operation relieves the implementation from being concerned with the following problem, and its variations.
\begin{enumerate}
    \item A thread $t_i$ is performing an insertion of a key $k$.
    \item Another thread $t_m$ is performing a migration to a newly allocated index.
    \item $t_i$ reaches its linearization point before the migration completes.
    \item Has $k$ been migrated?
\end{enumerate}
Instead, it becomes much easier to implement, and maintain, a logic in which thread $t_m$ initiates a Synchronous Operation, so that all other threads either complete their mutation before the migration starts, or know that a migration is in progress and help carry it out before resuming their work.
This is effectively the same approach expressed in~\cite[\S5.3.2, Preventing Concurrent Updates to Ensure Consistency]{maier}.

One may argue that, in principle, updates (as opposed to inserts) don't need to wait for a migration to complete, since all they need to do is to update the value pointer in the relevant data table entry.
Nevertheless, the update might need to be retried in case it returns that the searched key was not found, and if a concurrent migration completed in the meantime.
Additionally, a migration might in fact leave the data table untouched only if it happened to be a grow migration.
A compact, or shrink migration will instead try to reduce the size of the pages array of the data table, thus actually performing some operations on both the data table and the index.
Depending on the migration's current state, it may be that the update is referred by an index node to a wrong entry in the data table.
Therefore, updates are obstructed by migrations, in the same manner as inserts.

\subsection{Metadata}\label{subsec:metadata}
Most of the \texttt{AtomicDict} data is stored inside its metadata, which holds references to the index, and to the data table, among other useful things.
An external observer may perceive a migration as a change from a prior metadata object to a new one.
We will also refer to them as the current generation metadata and its successor, or the new generation metadata.

In addition to the aforementioned pointers, which are the core parts of metadata objects, are also stored therein:
\begin{enumerate}
    \item the size of the hash table (a power of 2);
    \item the pages counters (see~\S\ref{sec:approximate-size});
    \item the \texttt{is\_compact} flag;
    \item various information about the sizes of index nodes and their internal parts (see Table~\ref{tab:nodes});
    \item a pointer to the next metadata generation;
    \item the migration leader thread id (also see~\S\ref{sec:free-threading});
    \item the next node to migrate counter (or, in Maier's terminology, the blocks counter, that is, the next node to migrate is effectively the first node in what Maier denotes as a block);
    \item three atomic events used to signal the progress of the three phases of migrations (one metadata's events are only used for the migration which sees it as the current generation, since they are single-use by design, also see~\S\ref{sec:free-threading}); and
    \item other miscellaneous information.
\end{enumerate}

The metadata object is stored inside an \texttt{AtomicRef}, described in detail in Appendix~\ref{ch:atomic-reference}.
It essentially provides an interface for an atomically updatable reference to a \texttt{PyObject}, that also handles correct \texttt{PyObject} concurrent reference counting.
Specifically, the objects stored therein are subject to QSBR, and thus are not immediately freed upon reaching a reference count of 0.
See~\S\ref{sec:qsbr} for a discussion on QSBR\@.
This behavior is important for metadata objects, because it may be that a thread $t$ reads the pointer to the current metadata, and a concurrent migration replaces it with a newer generation, before $t$ manages to increment the metadata's reference count.
Thus, not having QSBR for metadata objects would make the implementation of \texttt{AtomicDict} subject to use-after-free bugs.

\subsection{Migration Leader}\label{subsec:migration-leader}

\subsection{Migration Phases}\label{subsec:migration-phases}


Mutation operations are blocked while a migration is in progress.
This is done by starting a Synchronous Operation.
The first phase of a migration is to prepare the new generation metadata.
\begin{itemize}
    \item In a shrink or compact migration, the metadata is ready after the leader thread allocates it, creates the new index, clears it (i.e.\ \texttt{memset(\ldots, 0, \ldots)}), allocates the new, smaller pages array, and copies the non-deleted pages into the new pages array.
    \item In a grow migration, the metadata is ready after the leader thread allocates it, creates the new index, and copies the pages array into a new, larger pages array.
\end{itemize}
After the new generation metadata is ready, the follower threads may start migrating the nodes from the current generation to the new one.

The Synchronous Operation is here ended by the leader thread for shrink and compact migrations, in order to allow new accessors to join the migration process.
Whereas for grow migrations, this is not done, and instead the Synchronous Operation is ended only after the current generation metadata is swapped with the new one.
The reason for this limitation is that the threads that participate in a grow migrations essentially need to hit a barrier and wait for all participants to have hit it.
In order for this to work, we need to know in advance the amount of threads that will need to hit the barrier.
Thus, we cannot allow the possibility of new accessors joining the migration process after it has started.
This is not necessary for shrink and compact migrations because they use a different semantic in order to determine whether the migration has completed.
Namely, that the generation tag of each page in the data table is set to the new generation.

\subsection{Slow Migrations}\label{subsec:slow-migrations}

As it can be easily evinced from the rest of this Section the migration process is essentially split into shrinks and compacts, or grows.
We refer to these two categories as slow and quick migrations henceforth.

The slow migration is conceptually simpler than the quick counterpart: what chiefly happens is that migrating threads compete to lock data pages for themselves and migrate the valid entries therein, i.e.\ those entries that have not been deleted, into the new generation index.
This is done by using the common insertion routine.
After the migration of a page is completed, the thread that previously locked the page sets the page's generation to the new generation.
To improve contention on the acquisition of pages, each thread, identified by the number $i$, starts migrating pages from the $i \mod P_A$ page.
After having visited all pages, it runs a second visit to check if all of the pages' generations have been set to the new generation.
If it does observe that, then it signals to the other accessors that the node migration phase is complete.

A call to the \texttt{compact} method triggers a compact migration, and such is the only mean to trigger a compact migration, but then, if the new generation meta is still not compact, further grow migrations are triggered, until a generation is reached s.t.\ the resulting meta is compact.

\subsection{Quick Migrations}\label{subsec:quick-migrations}
During the quick migration instead, a participating thread sets a local ``participation'' flag to 1 when joining the migration process (initially set to 0 by the leader for all accessors).
The node migration phase is then very similar to the one described in~\cite[\S5.3.1]{maier}.

A global counter is kept in the current generation metadata that specifies the next node to migrate.
A thread runs an atomic fetch-and-add operation to reserve a certain block, i.e.\ a portion of index to migrate.
By default the block size is 4096 index slots, consistent with the implementation of~\cite{maier}.
So that it is possible for a single thread to perform the entire migration process, if the current index size is sufficiently small.
In fact, the additional contention generated for smaller block sizes is not worth the blocking of the other threads.
After a block is reserved, the thread that made such reservation is free to not use atomic operations to migrate the nodes involved, because it effectively reserved both a portion in the current generation index, and in the new generation index.

Consider a block of size 4096 in the current generation index that starts at position $a$, i.e.\ $[a, a + 4096]$.
By~\cite[Lemma~5.1]{maier}, we know that the involved nodes must be migrated within the $[2a, 2(a + 4096 + 1)]$ block of the new generation index.
That is, grow migrations are only allowed to use a growth factor $\gamma = 2$.
By the semantics of the atomic fetch-and-add operation employed, it is known that $a$ is distinct among all participating threads, therefore the blocks in the current generation are distinct, as well as their counter parts in the new generation.
Thus, there is no need for further synchronization between migrating threads than the fetch-and-add operation.

The migrating thread, having reserved a block, clears the relevant portion of the new generation index and proceeds with inserting the same nodes into the new generation index (i.e.\ nodes pointing to the same location in the data table, albeit possibly differently sized and at different positions), skipping over tombstone nodes, and trying to maintain the Robin Hood invariants, falling back to linear probing if that is not possible.
Note that there not being a necessity to use 16-byte aligned CAS operations, the Robin Hood invariants are maintained to a larger extent, compared to the normal insertion routine.
After being done, the participating thread hits a barrier to synchronize the migration phase with the other threads.

After the node migration completes, the leader thread, swaps out the current metadata for the new one and signals to the other threads that the migration has finished.
This applies for both the quick and slow migrations.


\section{Approximate size}\label{sec:approximate-size}

An approximation of the number of elements stored in the hash table is inferred by the number of pages present.
Three separate, atomic counters are kept:

\begin{itemize}
    \item the greatest allocated page $P_A$;
    \item the greatest deleted page $P_D$; and
    \item the greatest refilled page $P_R$.
\end{itemize}

The $P_A$ counter is also used to keep track of the used portion of the pages array, and is the one used when a thread makes a new reservation into the data table; possibly, triggering the allocation of a new page and increasing $P_A$.

The three counters serve to provide a rough estimate on the number of elements currently stored: as new elements are inserted new data table pages need to be allocated, and when numerous elements are deleted they get defragmented into their own pages, before the pages are eventually freed, or refilled.
Initially, their respective values will be 0, -1, and -1.

So an upper bound for the number of elements contained in the dictionary is:
\[
    (P_A - P_D + P_R) \times |P|
\]

The bound is then also refined by visiting the $P_A$ and $P_R$ pages, decrementing it of the amount of entries that weren't filled in those pages.
There is also a lower bound, which is computed by decrementing the upper bound by the unused reservations, accounted by traversing the accessors' storages.
Therefore, the running time complexity of this operation is $\Theta(t)$.

The mean (floor division by 2) of the lower and upper bounds is then reported to the user upon request.

This routine is also employed by the insert and delete routines to decide when to trigger migrations.


\section{Consistent size}\label{sec:consistent-size}

To provide a sequentially consistent computation of the hash table size, each accessor has a thread-local size counter: each thread keeps track of how many items it has inserted, incrementing its local counter, and how many items it has deleted, decrementing its local counter.
(Thus, it may be that one thread's local counter is negative.)
The thread-local counters are mutated using regular reads and writes: they are protected by the accessor's lock.

Upon request, a Synchronous Operation to safely sum the thread-local counters, begins.
The local counters are reset to 0 before the operation ends.
The result is cached into the \texttt{AtomicDict} and a dirty flag is reset in order to avoid restarting a Synchronous Operation at every call, if no insertions or deletions marked the cached value as dirty.
The cached value is stored with a CPython integer.
Upon a new request, if the dirty flag is unset, the cached value is returned.

Note that it is much faster to keep thread-local counters and then acquire locks to read them, rather than to keep a shared counter.
Using a Synchronous Operation for this also implies that its semantics are trivially sequentially consistent.
It can be argued that to retrieve this consistent size, there is a missed opportunity cost related to the exclusion of all other threads from using the hash table.
While true in principle, it should be noted that this is a $\Theta(t)$ operation, thus independent of the table size, and generally quite fast.
For these reasons, this is the routine that is exposed by default, i.e.\ when calling \texttt{{len(AtomicDict())}}.
If a program cannot withstand the added cost of mutual exclusion, perhaps because it frequently asks for the size of the hash table while this is being concurrently mutated, it is still possible to retrieve the approximate size described in the above Section, which is quite accurate and lock-free.
Under high-frequency concurrent mutations, it may make more sense to retrieve an approximation of the size rather than a sequentially consistent size: it may be invalidated immediately upon returning the output, so it would never actually be precise.

Notwithstanding the limitation on the number of nodes that can be present into the described hash table at any given time ($2^{56}$), as prescribed by Table~\ref{tab:nodes}, a counter of 64-bits is not necessarily sufficient.
Consider the situation where two threads concurrently insert and delete $2^{64}$ items, so that one thread only performs insertions, and the other only performs deletions (or any other equivalent situation).
Care then needs to be taken in order to avoid running into integer overflows or underflows, a notorious problem in C code.
Before such an event occurs, the thread that detects it (its local counter is \texttt{MAX\_INT64} during an insert, or \texttt{MIN\_INT64} during a delete), the thread repeatedly tries to add (CAS) its local counter to the \texttt{AtomicDict}'s cached value, while holding its local accessor's storage mutex.
This way, no Synchronous Operation can begin, and it is safe to add the protected value, before resetting it to 0.
If another thread finds itself in the same condition, the competing threads will serialize based on the CAS to the cached counter.


\section{Batch lookup}\label{sec:batch-lookup}

Inspired by the design of DRAM-HiT, this method was added so as to amortize the cost of multiple memory accesses when a program wants to read a number of keys from the hash table.
It deviates from~\cite{dramhit} in that it does not require the user to continuously poll the table for results.
Instead, a batch of keys can be submitted for lookup, in one invocation.

An example call:

\begin{lstlisting}[language=Python,label={lst:batch-lookup-usage}]
foo = AtomicDict({'a': 1, 'b': 2, 'c': 3})
result = foo.batch_getitem({
  'a': None,
  'b': None,
  'f': None,
})
assert result == {
  'a': 1,
  'b': 2,
  'f': cereggii.NOT_FOUND,
}
\end{lstlisting}

The \texttt{cereggii.NOT\_FOUND} object is a special, global object that cannot be inserted into \texttt{AtomicDict} (as previously described in Section~\ref{sec:insert-or-update}).
The fact that it is in the output means, as its name implies, that the key was not in the dictionary.

The operation involves prefetching each distance-0 location of the keys in the batch.
This results in an amortization of the cost of individual memory accesses: if the number of prefetches issued is sufficiently large (how large depends on the hardware), then the processor will be stalled waiting for the first read, while the subsequent ones will be read from the cache without incurring in the cost of main memory access.
This behavior is cheaper-by-comparison: it is cheaper to prefetch several keys, possibly blocking at the first one, than to block at every read.

The submitted batch is subdivided into chunks of a configurable size.
This is intended to prevent memory over-fetching.
That is, the cache of an individual processor may become overly filled with the prefetched portions of the index, and the hardware will proceed to evict lines of cache before they are actually read by the lookups that requested them.

NUMA architectures will enjoy this mechanism particularly: when prefetches are issued at remote memory addresses, the comparative cost of the amortized reads is much less than the sum of the costs of each individual read.

Concerning NUMA architectures, it is generally also possible to argue that the cost of a remote access may be hidden by the possibly many local memory accesses that precede it.
Such is not our case: the index, which is allocated as a contiguous array, will reside entirely in a single NUMA node.
Thus, a thread accessing the index will either only issue operations to remote memory, or to local memory.
NUMA support is thus limited: the presence of a single shared index makes it impossible to efficiently make use of memory partitioning.

\paragraph{Sharding.}
A further endeavor may be carried out so as to provide an \texttt{Atomic\-ShardedDict} class which creates its own pool of threads, bound to individual NUMA nodes, and affords access to the hash table shards to those threads only.
The partitioning of a hash table into several shards requires that any given key must be uniquely assigned to one shard (e.g.\ by using the least-significant bits of the hash), each of which is managed by one or multiple threads associated with a single NUMA node.
Mimicking the design of DRAM-HiT, the user thread would submit requests to a message queue which is consumed by the threads associated with the relevant shard.
This is resembling of the design of~\cite{dramhit}, though this design need not be asynchronous in order to gain the speed-up of stronger memory associativity.
It can be implemented by making use of the current implementation of \texttt{AtomicDict}, such that one shard of \texttt{AtomicShardedDict} is an instance of \texttt{AtomicDict}.

\subsection{Linearization}\label{subsec:batch-lookup-linearization}

The linearization of this method is intended as the individual linearizations of the distinct lookup operations.
Each distinct lookup is linearizable, although the overall invocation is not.
Therefore, it is possible for a result of this method to not be sequentially consistent.


\section{Reduce}\label{sec:reduce}

A common pattern when using CAS is to repeatedly make the call to the CAS routine in order to cope with concurrent mutations to the contended area of memory.

We are especially interested into efficiently and ergonomically handle the mutation of the value of a given key, for such is an important use-case for hash tables in general.

We propose here an interface that is believed to satisfy both the efficiency and the ergonomics goals, which is also very similar to~\cite[Algorithm~1]{maier}.
Instead of providing an expected and a desired value, the programmer specifies the desired mutation, i.e.\ a function over the current value.
Consider the following piece of Python code.
\begin{lstlisting}[label={lst:aggregate-usage}, language=Python]
	d = AtomicDict({"spam": 1})

	d.aggregate(key="spam",
	  mutation=lambda current_value:
	    1 if current_value is NOT_FOUND else current_value + 1
	)
\end{lstlisting}
The \texttt{lambda} expression on line 4 is a Python idiom to define an anonymous function.

A key may be already present or not; thus, a programmer using this method should be instructed to write a function that handles both cases, and that there should be no assumptions as to the number of times this function is called before its described mutation is actually applied to the value.
That is, it may be that the function is called multiple times, due to contention.

That function is called, possibly multiple times, by a routine that behaves like the following:
\begin{lstlisting}[capition={A common pattern when using CAS instructions.}, label={lst:aggregate}, language=Python]
	expected = d.get(key, cereggii.NOT_FOUND)
	while True:
	  try:
	    d.compare_and_set(key, expected, mutation(expected))
	  except cereggii.ExpectationFailed:
	    expected = d.get(key, cereggii.NOT_FOUND)
	  else:
	    break
\end{lstlisting}
Such routine would likely be implemented in C code, for the sake of uniformity with the rest of the implementation of \texttt{AtomicDict}.

The specific method of calling CAS in the above Listing is very commonly known in the literature, albeit certainly more common to be written in a form where the CAS operation returns a boolean, rather than raising an exception.
(See also the relevant discussion in Section~\ref{sec:insert-or-update}.)
The fact that this is well known in the literature, though, does not directly imply that it is also well known in the literacy of Python programmers.
For the many programmers who are not accustomed to the domain of concurrency, the above Listing should probably be explained in detail, before its semantics are appreciated.
Providing a transparent and standardized way to access such common knowledge is therefore considered fruitful.

Do note that the atomicity of this \texttt{aggregate} method is violated if within the provided mutation function, a lookup into \texttt{d} is performed.
Or in other words, this is not a suitable design for a mutation that requires a sequentially consistent view of more than one key in the dictionary.

In order to provide further performance improvements, another method was added, already known in computing as \emph{reduce}.
Such is a foundational concept of many computing models, most notably those based around the concept of map-reduce like Hadoop.
As such, it could easily be provided based on the above \texttt{aggregate} method, and further enhanced to minimize contentious access to the dictionary.
Its usage would be something resembling the following lines of code:
\begin{lstlisting}[label={lst:reduce-usage}, language=Python]
	d = AtomicDict()

    data = [
        ("red", 1),
        ("green", 42),
        ("blue", 3),
        ("red", 5),
    ]

    def count(key, current, new):
        if current is cereggii.NOT_FOUND:
            return new
        return current + new

    d.reduce(data, count)
\end{lstlisting}

The parameter \texttt{key} is added so as to satisfy the possible cases in which the computation is dependent on the key itself.

Contention can be greatly reduced by first accumulating an intermediate result into a thread-local hash table, and then applying the mutation function to the shared hash table.
This reduces contention because the repetition of keys in the input is not made visible to other threads, but only to the thread-local hash table; that is, this operation requires $\Omega(|K|) \ll \Omega(n)$ atomic writes to memory.
As keys in the data are repeated more frequently, the number of atomic writes required decreases.
In fact, a piece of real-world data is often found to be skewed towards a small subset of its own keys.

Finally, since this thread-local hash table certainly requires allocation of memory, its size could also be bound.
Upon reaching the bound, the accumulated intermediate result is applied to the shared table, and then the operation is repeated until the exhaustion of \texttt{iterator}.

\paragraph{Linearization and lock-freedom.}
The linearization and lock-freedom of \texttt{reduce} chiefly depends on the respective properties of the \texttt{ExpectedInsertOrUpdate} routine that is being called in order to implement it.
Thus, refer to Sections~\ref{subsec:insert-linearization} and~\ref{subsec:insert-lock-freedom}.


\section{Iterations}\label{sec:iterations}

Iterations are designed to be particularly efficient in \texttt{AtomicDict}, mostly because of the design of the data table, which helps with cache efficiency.
Speculative hardware memory pre-fetching can also help by observing the linear access pattern.
(In~\cite{maier}, iterations are referred to as \emph{Forall} operations.)
Additionally, the partitioning scheme described in the following Section enables truly multi-threaded iterations, by creating per-partition iterators.
This is something that is not available in any other reviewed hash table implementation.

The presented iteration scheme, accessible through the \texttt{{fast\_iter}} method, has been showed to perform twice as fast as CPython's \texttt{dict} iterator, even for single threaded executions.
(See Section~\ref{subsec:comparisons-results} for measures.)

Iterating is done primarily over the data table.
A thread starts by fetching the first page, then (skipping over empty or deleted cells) reads the first entry of the page, yielding the item back to the caller, after storing the next location to visit.
The Python code then loops calling this same function again.
This requires the creation of a small iterator \texttt{PyObject} to keep track of the next location to visit, a necessity for adhering to the CPython loop protocol regardless of our designs.

\subsection{Iteration Partitioning}\label{subsec:iteration-partitioning}

The above iteration scheme is further enhanced with partitioning.
When multiple threads want to iterate over the same hash table, it may very well make sense that they visit disjoint subsets of the stored key-set.

This is achieved by letting the thread declare the number of partitions $\pi$, and the partition number $i$ assigned to it (a thread identifier of some sort), s.t. $i \in [0, \pi)$.
For the partition to actually yield disjoint subsets, $\pi$ has to be known to all participating threads, and is intended to be equal to their number, and $i$ needs to be assigned uniquely to each thread.
A call to \texttt{{fast\_iter(partitions=$\pi$, this\_partition=$i$)}} will then visit those pages for which their page number $p \mod \pi = i$.

The number of keys per subset is not guaranteed to be to be uniformly distributed.
Nevertheless, if the key-set is large enough (that is, $|K| \gg \pi \times |P|$), and the number of deleted keys per-page is nearly constant on average, then the distribution of keys per partition is also nearly constant on average.
(That is, let $\mu = E[d(p)]$ and $\sigma$ its standard deviation, $\sigma \ll \mu$.)

\subsection{Non-linearization and lock-freedom}\label{subsec:iteration-linearization-lock-freedom}
While the lock-freedom property this operation enjoys is trivial, its linearization is trickier.
This iteration is in fact not guaranteed to be consistent with any sequentially consistent access, and is therefore not considered linearizable.
In fact, repeated calls to ask for the next element in the iterator are required in order to comply with CPython's iteration protocol.
As such, it cannot be that ``[the] method call [appears] to take effect instantaneously at some moment between its invocation and response''~\cite[Principle~3.5.1]{art-mp}

It is therefore advised to use this faster iterator when it is somehow known that there will be no concurrent mutations, by the logic of the program.
Otherwise, the following sequentially inconsistent behaviors might be observed:
\begin{enumerate}
    \item the same key is emitted more than once; or
    \item an update $u_1$ that happened strictly before another $u_2$ is not seen, while the latter is.
    (Such is the case when two updates are executed in succession by the same thread.
    Thus, it is definitely known that $u_1 \rightarrow u_2$.)
\end{enumerate}

These may be very surprising behaviors for a programmer trying to debug an incorrect execution of an iterating thread.

Those behaviors may be exhibited in the following circumstances, respectively:
\begin{enumerate}
    \item the iterating thread $t_l$ visits the data table entry $e$ in which some key $k$ is stored $\rightarrow$ another thread $t_d$ deletes $k$ $\rightarrow$ another thread $t_i$ re-inserts $k$ at a location $e'$ in the data table s.t.\ $e < e'$ $\rightarrow$ $t_l$ visits $e'$ in which $k$ is stored.
    (It may also be that $t_d \equiv t_i$.)
    \item the iterating thread $t_l$ visits the data table at location $e$ where the key $k_1$ is stored $\rightarrow$ update $u_1$ to key $k_1$ is performed by another thread $t_{u_1}$ $\rightarrow$ update $u_2$ to key $k_2$, stored at location $e'$ s.t. $e < e'$ is performed by another thread $t_{u_2}$ $\rightarrow$ $t_l$ visits the data table location $e'$, seeing the effect of update $u_2$.
    (It may also be that $t_d \equiv t_i$.)
\end{enumerate}


\section{Proposal for a consistent iteration}\label{sec:consistent-iteration}

The following proposal for a consistent iteration has not been implemented as of the time of writing, but it is fully described here.
Once implemented, it will be the default iteration scheme adopted, given that its semantics are much more immediate to understand.
To implement a cost-efficient and consistent iteration seems to be possible, based on the following design:

\begin{enumerate}
    \item the iterating thread $t$ begins a Synchronous Operation (\S\ref{sec:synchronous-operations});
    \item it generates an iteration ID $i$ (the pointer to the C structure holding the iteration data);
    \item it allocates a new iteration index and copies the shared index into it;
    \item it allocates a new iteration pages array;
    \item it traverses the \texttt{AtomicDict}'s pages array, and for each page:
    \begin{enumerate}
        \item it sets the \texttt{iteration} field on the page to $i$;
        \item it copies the page pointer into the local pages array;
        \item it increments the page reference count;
    \end{enumerate}
    \item $t$ ends the Synchronous Operation; and
    \item proceeds iterating over the new pages, in a manner similar to the iteration described in Section~\ref{sec:iterations}.
    \item When the iteration completes, it decrements the reference count of the pages in the local pages array, and frees the array.
\end{enumerate}

When the iteration field of a page is set, the page is considered to be immutable.
A thread that wishes to mutate it, should thus first copy the page's contents into a newly allocated page, and then atomically swap the two pages in the pages array.
If the swap fails, then it means that another thread has swapped the page into a mutable page.
It cannot be that the newly swapped-in page is immutable because that requires holding all of the accessors' locks.

The creation of new mutable pages is therefore entirely offloaded, from the iterating thread, to the other \texttt{AtomicDict}'s accessors.
This is a feature, rather than a compromise, of this design.
Consider the opposite case in which the iterating thread additionally has to allocate more pages and swap them in the pages array, or equivalently has to create its own immutable pages.
In this scenario the Synchronous Operation itself would take much longer to complete.
Thus, it would hinder the performance of the other accessors which are trying to apply mutations to the hash table (recall that lookups are permitted in spite of Synchronous Operations).
That is, there is a missed opportunity cost associated with such design.

Instead, the parallelization of threads which independently swap the immutable pages out for their mutable copies, entails that multiple threads can be used to create the new pages.
And, furthermore, that if no mutations were actually going to happen, either at all or in a subset of the pages, then strictly less work is required overall.

It may also be entirely possible to permit the re-usage of the iteration ID\@.
So that if a thread sees that all pages refer to the same iteration ID, then it avoids marking the pages as immutable, and instead copies the associated data into its own iterator.

\paragraph{Linearization and non-lock-freedom.}
With this being a Synchronous Operation, it is trivially not lock-free.
Its linearization also follows the semantics of Synchronous Operations: the iterator linearizes when it manages to acquire all the required locks.
